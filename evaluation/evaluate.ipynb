{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_path (model):\n",
    "    filePath=f\"/Users/sden118/Desktop/FinReasoning/test/{model}_test.json\"\n",
    "    return filePath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "    a=data[\"isCorrect\"].sum()\n",
    "    b=data['isCorrect'].count()\n",
    "    accuracy=(a/b*100).__round__(2)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    # data=data[data['Question Type']==\"text+image\"]\n",
    "    \n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "    a=data[\"isCorrect\"].sum()\n",
    "    b=data['isCorrect'].count()\n",
    "    overall=(a/b*100).__round__(2)\n",
    "\n",
    "    df1=data[data[\"QA Type\"]==\"Knowledge Reasoning QA\"]\n",
    "   \n",
    "    c=df1[\"isCorrect\"].sum()\n",
    "    d=df1['isCorrect'].count()\n",
    "    expertise=(c/d*100).__round__(2)\n",
    "\n",
    "    df2=data[data[\"QA Type\"]==\"Math Reasoning QA\"]\n",
    "    \n",
    "    e=df2[\"isCorrect\"].sum()\n",
    "    f=df2['isCorrect'].count()\n",
    "    math=(e/f*100).__round__(2)\n",
    "    return overall,expertise,math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topics(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    # 计算每个答案是否正确\n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "\n",
    "    # data['General Topics']=data['General Topics'].apply(lambda x: 'Others' if x in other_topics  else x)\n",
    "\n",
    "    # 计算每个 General Topics 的正确答案数量\n",
    "    correct_by_topic = data.groupby('General Topics')['isCorrect'].sum().reset_index()\n",
    "\n",
    "    # 计算每个 General Topics 的总答案数量\n",
    "    count_by_topic = data.groupby('General Topics')['isCorrect'].count().reset_index()\n",
    "\n",
    "    # 合并两个 DataFrame\n",
    "    accuracy_by_topic = pd.merge(correct_by_topic, count_by_topic, on='General Topics')\n",
    "\n",
    "    # 计算正确率\n",
    "    accuracy_by_topic['accuracy%'] = round(accuracy_by_topic['isCorrect_x'] / accuracy_by_topic['isCorrect_y'] * 100, 2)\n",
    "\n",
    "    # 重命名列\n",
    "    accuracy_by_topic.columns = ['General Topics', 'correct', 'total', 'accuracy%']\n",
    "\n",
    "    # 计算总体正确率\n",
    "    total_correct = data['isCorrect'].sum()\n",
    "    total_count = data['isCorrect'].count()\n",
    "    overall_accuracy = round((total_correct / total_count) * 100, 2)\n",
    "\n",
    "    # 创建一个新的 DataFrame 来存储总体正确率\n",
    "    overall_accuracy_df = pd.DataFrame([['Overall', total_correct, total_count, overall_accuracy]], columns=['General Topics', 'correct', 'total', 'accuracy%'])\n",
    "\n",
    "    # 将总体正确率 DataFrame 与 accuracy_by_topic 进行合并\n",
    "    final_df = pd.concat([accuracy_by_topic, overall_accuracy_df], ignore_index=True)\n",
    "\n",
    "    # 打印结果\n",
    "    accuracy=list(final_df['accuracy%'])\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topics_test(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    # data=data[data['Question Type']==\"text+image\"]\n",
    "    # 计算每个答案是否正确\n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "\n",
    "    # data['General Topics']=data['General Topics'].apply(lambda x: 'Others' if x in other_topics  else x)\n",
    "\n",
    "    # 计算每个 General Topics 的正确答案数量\n",
    "    correct_by_topic = data.groupby('General Topics')['isCorrect'].sum().reset_index()\n",
    "\n",
    "    # 计算每个 General Topics 的总答案数量\n",
    "    count_by_topic = data.groupby('General Topics')['isCorrect'].count().reset_index()\n",
    "\n",
    "    # 合并两个 DataFrame\n",
    "    accuracy_by_topic = pd.merge(correct_by_topic, count_by_topic, on='General Topics')\n",
    "\n",
    "    # 计算正确率\n",
    "    accuracy_by_topic['accuracy%'] = round(accuracy_by_topic['isCorrect_x'] / accuracy_by_topic['isCorrect_y'] * 100, 2)\n",
    "\n",
    "    # 重命名列\n",
    "    accuracy_by_topic.columns = ['General Topics', 'correct', 'total', 'accuracy%']\n",
    "\n",
    "    # 计算总体正确率\n",
    "    total_correct = data['isCorrect'].sum()\n",
    "    total_count = data['isCorrect'].count()\n",
    "    overall_accuracy = round((total_correct / total_count) * 100, 2)\n",
    "\n",
    "    # 创建一个新的 DataFrame 来存储总体正确率\n",
    "    overall_accuracy_df = pd.DataFrame([['Overall', total_correct, total_count, overall_accuracy]], columns=['General Topics', 'correct', 'total', 'accuracy%'])\n",
    "\n",
    "    # 将总体正确率 DataFrame 与 accuracy_by_topic 进行合并\n",
    "    final_df = pd.concat([accuracy_by_topic, overall_accuracy_df], ignore_index=True)\n",
    "\n",
    "    # 打印结果\n",
    "    accuracy=list(final_df['accuracy%'])\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[\n",
    "        \"claude_sonnet\",\"claude_sonnet_rag\",\n",
    "        \"gpt_o1\",\"gpt_o1_rag\",\n",
    "        \"Gemini_Pro1\",\"Gemini_Pro1_rag\",\n",
    "        \"deepseek\",\"deepseek_rag\",\n",
    "        \n",
    "        \"llama\",\"llama_rag\",\n",
    "        \"qwen\",\"qwen_rag\",\n",
    "        \n",
    "        \n",
    "        \"gpt_4o\",\"gpt_4o_rag\",\n",
    "        \"gemini_1.5_pro\",\"gemini_1.5_pro_rag\",\n",
    "        \"claude_3.5\",\"claude_3.5_rag\",\n",
    "        \"qwen2_vl\",\"qwen2_vl_rag\",\n",
    "        \"Llava\",\"Llava_rag\",\n",
    "        \"Llama_3.2_vision\",\"Llama_3.2_vision_rag\",\n",
    "        \n",
    "        ]\n",
    "topics=[\"Investment\",\n",
    "\"Quantitative Methods\",\n",
    "\"Valuation and Risk Models\",\n",
    "\"Financial Markets and Products\",\n",
    "\"Financial Reporting and Analysis\",\n",
    "\"Portfolio Management\",\n",
    "\"Fixed Income\",\n",
    "\"Foundation of Risk Management\",\n",
    "\"Credit Risk\",\n",
    "\"Economics\",\n",
    "\"Operational Risk\",\n",
    "\"Derivatives\",\n",
    "\"Market Risk\",\n",
    "\"Corporate Finance\",\n",
    "\"Liquidity and Treasury Risk\",\n",
    " 'Overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Overall%</th>\n",
       "      <th>Expertise%</th>\n",
       "      <th>Math%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude_sonnet</td>\n",
       "      <td>53.91</td>\n",
       "      <td>61.83</td>\n",
       "      <td>42.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude_sonnet_rag</td>\n",
       "      <td>64.84</td>\n",
       "      <td>71.24</td>\n",
       "      <td>55.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt_o1</td>\n",
       "      <td>46.56</td>\n",
       "      <td>55.65</td>\n",
       "      <td>33.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt_o1_rag</td>\n",
       "      <td>60.31</td>\n",
       "      <td>68.28</td>\n",
       "      <td>49.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini_Pro1</td>\n",
       "      <td>47.81</td>\n",
       "      <td>57.26</td>\n",
       "      <td>34.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gemini_Pro1_rag</td>\n",
       "      <td>61.37</td>\n",
       "      <td>68.45</td>\n",
       "      <td>51.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>61.25</td>\n",
       "      <td>57.53</td>\n",
       "      <td>66.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deepseek_rag</td>\n",
       "      <td>71.88</td>\n",
       "      <td>66.67</td>\n",
       "      <td>79.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama</td>\n",
       "      <td>27.34</td>\n",
       "      <td>30.11</td>\n",
       "      <td>23.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama_rag</td>\n",
       "      <td>36.09</td>\n",
       "      <td>38.98</td>\n",
       "      <td>32.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>qwen</td>\n",
       "      <td>55.62</td>\n",
       "      <td>57.53</td>\n",
       "      <td>52.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>qwen_rag</td>\n",
       "      <td>67.97</td>\n",
       "      <td>67.74</td>\n",
       "      <td>68.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>72.19</td>\n",
       "      <td>73.17</td>\n",
       "      <td>71.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt_4o_rag</td>\n",
       "      <td>81.72</td>\n",
       "      <td>81.03</td>\n",
       "      <td>83.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>70.83</td>\n",
       "      <td>71.24</td>\n",
       "      <td>70.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemini_1.5_pro_rag</td>\n",
       "      <td>82.06</td>\n",
       "      <td>81.18</td>\n",
       "      <td>83.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>claude_3.5</td>\n",
       "      <td>75.94</td>\n",
       "      <td>77.96</td>\n",
       "      <td>73.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>claude_3.5_rag</td>\n",
       "      <td>80.78</td>\n",
       "      <td>81.18</td>\n",
       "      <td>80.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qwen2_vl</td>\n",
       "      <td>52.66</td>\n",
       "      <td>59.14</td>\n",
       "      <td>43.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen2_vl_rag</td>\n",
       "      <td>65.00</td>\n",
       "      <td>70.70</td>\n",
       "      <td>57.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Llava</td>\n",
       "      <td>16.72</td>\n",
       "      <td>18.55</td>\n",
       "      <td>14.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Llava_rag</td>\n",
       "      <td>28.28</td>\n",
       "      <td>29.30</td>\n",
       "      <td>26.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Llama_3.2_vision</td>\n",
       "      <td>19.38</td>\n",
       "      <td>23.39</td>\n",
       "      <td>13.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Llama_3.2_vision_rag</td>\n",
       "      <td>27.19</td>\n",
       "      <td>30.91</td>\n",
       "      <td>22.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Overall%  Expertise%  Math%\n",
       "0          claude_sonnet     53.91       61.83  42.91\n",
       "1      claude_sonnet_rag     64.84       71.24  55.97\n",
       "2                 gpt_o1     46.56       55.65  33.96\n",
       "3             gpt_o1_rag     60.31       68.28  49.25\n",
       "4            Gemini_Pro1     47.81       57.26  34.70\n",
       "5        Gemini_Pro1_rag     61.37       68.45  51.49\n",
       "6               deepseek     61.25       57.53  66.42\n",
       "7           deepseek_rag     71.88       66.67  79.10\n",
       "8                  llama     27.34       30.11  23.51\n",
       "9              llama_rag     36.09       38.98  32.09\n",
       "10                  qwen     55.62       57.53  52.99\n",
       "11              qwen_rag     67.97       67.74  68.28\n",
       "12                gpt_4o     72.19       73.17  71.43\n",
       "13            gpt_4o_rag     81.72       81.03  83.08\n",
       "14        gemini_1.5_pro     70.83       71.24  70.26\n",
       "15    gemini_1.5_pro_rag     82.06       81.18  83.27\n",
       "16            claude_3.5     75.94       77.96  73.13\n",
       "17        claude_3.5_rag     80.78       81.18  80.22\n",
       "18              qwen2_vl     52.66       59.14  43.66\n",
       "19          qwen2_vl_rag     65.00       70.70  57.09\n",
       "20                 Llava     16.72       18.55  14.18\n",
       "21             Llava_rag     28.28       29.30  26.87\n",
       "22      Llama_3.2_vision     19.38       23.39  13.81\n",
       "23  Llama_3.2_vision_rag     27.19       30.91  22.01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAcc=[]\n",
    "expertiseAcc=[]\n",
    "mathAcc=[]\n",
    "for model in models:\n",
    "    a,b,c=evaluate_test(file_path(model))\n",
    "    modelAcc.append(a)\n",
    "    expertiseAcc.append(b)\n",
    "    mathAcc.append(c)\n",
    "result={\n",
    "    'Model': models,\n",
    "    'Overall%': modelAcc,\n",
    "    'Expertise%': expertiseAcc,\n",
    "    'Math%': mathAcc\n",
    "\n",
    "}\n",
    "\n",
    "result_test=pd.DataFrame(result)\n",
    "result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test.to_csv('/Users/sden118/Desktop/FinReasoning/evaluation/Model_accuracy_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>claude_sonnet</td>\n",
       "      <td>53.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>claude_sonnet_rag</td>\n",
       "      <td>64.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt_o1</td>\n",
       "      <td>46.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt_o1_rag</td>\n",
       "      <td>60.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini_Pro1</td>\n",
       "      <td>47.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gemini_Pro1_rag</td>\n",
       "      <td>61.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>61.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deepseek_rag</td>\n",
       "      <td>71.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>llama</td>\n",
       "      <td>27.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>llama_rag</td>\n",
       "      <td>36.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>qwen</td>\n",
       "      <td>55.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>qwen_rag</td>\n",
       "      <td>67.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>72.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpt_4o_rag</td>\n",
       "      <td>81.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>70.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gemini_1.5_pro_rag</td>\n",
       "      <td>82.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>claude_3.5</td>\n",
       "      <td>75.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>claude_3.5_rag</td>\n",
       "      <td>80.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qwen2_vl</td>\n",
       "      <td>52.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen2_vl_rag</td>\n",
       "      <td>65.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Llava</td>\n",
       "      <td>16.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Llava_rag</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Llama_3.2_vision</td>\n",
       "      <td>19.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Llama_3.2_vision_rag</td>\n",
       "      <td>9.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy%\n",
       "0          claude_sonnet      53.91\n",
       "1      claude_sonnet_rag      64.84\n",
       "2                 gpt_o1      46.56\n",
       "3             gpt_o1_rag      60.31\n",
       "4            Gemini_Pro1      47.81\n",
       "5        Gemini_Pro1_rag      61.37\n",
       "6               deepseek      61.25\n",
       "7           deepseek_rag      71.88\n",
       "8                  llama      27.34\n",
       "9              llama_rag      36.09\n",
       "10                  qwen      55.62\n",
       "11              qwen_rag      67.97\n",
       "12                gpt_4o      72.19\n",
       "13            gpt_4o_rag      81.72\n",
       "14        gemini_1.5_pro      70.83\n",
       "15    gemini_1.5_pro_rag      82.06\n",
       "16            claude_3.5      75.94\n",
       "17        claude_3.5_rag      80.78\n",
       "18              qwen2_vl      52.66\n",
       "19          qwen2_vl_rag      65.00\n",
       "20                 Llava      16.72\n",
       "21             Llava_rag      28.28\n",
       "22      Llama_3.2_vision      19.38\n",
       "23  Llama_3.2_vision_rag       9.04"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAcc=[]\n",
    "for model in models:\n",
    "    modelAcc.append(evaluate(file_path(model)))\n",
    "result={\n",
    "    'Model': models,\n",
    "    'Accuracy%': modelAcc\n",
    "}\n",
    "\n",
    "result=pd.DataFrame(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/Volumes/Jennie/Reasoning/FinReasoning/evaluation/Model_Accuracy.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemini_1.5_pro_rag</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt_4o_rag</td>\n",
       "      <td>85.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_o1_rag</td>\n",
       "      <td>80.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>79.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek_rag</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemini_Pro1_rag</td>\n",
       "      <td>78.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qwen_rag</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini_Pro1</td>\n",
       "      <td>70.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_o1</td>\n",
       "      <td>67.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qwen</td>\n",
       "      <td>66.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama_rag</td>\n",
       "      <td>47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama_3.2_vision_rag</td>\n",
       "      <td>36.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Llava_rag</td>\n",
       "      <td>35.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Llava</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Llama_3.2_vision</td>\n",
       "      <td>22.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy%\n",
       "11    gemini_1.5_pro_rag      86.00\n",
       "9             gpt_4o_rag      85.28\n",
       "1             gpt_o1_rag      80.41\n",
       "8                 gpt_4o      79.69\n",
       "5           deepseek_rag      78.75\n",
       "3        Gemini_Pro1_rag      78.68\n",
       "10        gemini_1.5_pro      77.53\n",
       "13              qwen_rag      76.68\n",
       "4               deepseek      70.90\n",
       "2            Gemini_Pro1      70.39\n",
       "0                 gpt_o1      67.67\n",
       "12                  qwen      66.02\n",
       "7              llama_rag      47.95\n",
       "17  Llama_3.2_vision_rag      36.88\n",
       "15             Llava_rag      35.47\n",
       "6                  llama      35.20\n",
       "14                 Llava      28.73\n",
       "16      Llama_3.2_vision      22.06"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='Accuracy%', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Investment</th>\n",
       "      <th>Quantitative Methods</th>\n",
       "      <th>Valuation and Risk Models</th>\n",
       "      <th>Financial Markets and Products</th>\n",
       "      <th>Financial Reporting and Analysis</th>\n",
       "      <th>Portfolio Management</th>\n",
       "      <th>Fixed Income</th>\n",
       "      <th>Foundation of Risk Management</th>\n",
       "      <th>Credit Risk</th>\n",
       "      <th>Economics</th>\n",
       "      <th>Operational Risk</th>\n",
       "      <th>Derivatives</th>\n",
       "      <th>Market Risk</th>\n",
       "      <th>Corporate Finance</th>\n",
       "      <th>Liquidity and Treasury Risk</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claude_sonnet</th>\n",
       "      <td>52.5</td>\n",
       "      <td>50.00</td>\n",
       "      <td>65.38</td>\n",
       "      <td>52.17</td>\n",
       "      <td>22.22</td>\n",
       "      <td>48.10</td>\n",
       "      <td>60.24</td>\n",
       "      <td>37.50</td>\n",
       "      <td>52.83</td>\n",
       "      <td>18.18</td>\n",
       "      <td>30.77</td>\n",
       "      <td>22.22</td>\n",
       "      <td>65.00</td>\n",
       "      <td>47.62</td>\n",
       "      <td>56.52</td>\n",
       "      <td>53.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_sonnet_rag</th>\n",
       "      <td>65.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>78.85</td>\n",
       "      <td>65.22</td>\n",
       "      <td>22.22</td>\n",
       "      <td>60.76</td>\n",
       "      <td>71.08</td>\n",
       "      <td>62.50</td>\n",
       "      <td>60.38</td>\n",
       "      <td>45.45</td>\n",
       "      <td>46.15</td>\n",
       "      <td>33.33</td>\n",
       "      <td>75.00</td>\n",
       "      <td>61.90</td>\n",
       "      <td>56.52</td>\n",
       "      <td>64.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_o1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>35.71</td>\n",
       "      <td>59.62</td>\n",
       "      <td>56.52</td>\n",
       "      <td>33.33</td>\n",
       "      <td>37.97</td>\n",
       "      <td>53.01</td>\n",
       "      <td>18.75</td>\n",
       "      <td>45.28</td>\n",
       "      <td>18.18</td>\n",
       "      <td>38.46</td>\n",
       "      <td>66.67</td>\n",
       "      <td>51.67</td>\n",
       "      <td>40.48</td>\n",
       "      <td>30.43</td>\n",
       "      <td>46.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_o1_rag</th>\n",
       "      <td>67.5</td>\n",
       "      <td>50.00</td>\n",
       "      <td>69.23</td>\n",
       "      <td>65.22</td>\n",
       "      <td>55.56</td>\n",
       "      <td>49.37</td>\n",
       "      <td>68.67</td>\n",
       "      <td>31.25</td>\n",
       "      <td>61.32</td>\n",
       "      <td>36.36</td>\n",
       "      <td>46.15</td>\n",
       "      <td>77.78</td>\n",
       "      <td>65.83</td>\n",
       "      <td>54.76</td>\n",
       "      <td>47.83</td>\n",
       "      <td>60.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1</th>\n",
       "      <td>47.5</td>\n",
       "      <td>28.57</td>\n",
       "      <td>57.69</td>\n",
       "      <td>60.87</td>\n",
       "      <td>33.33</td>\n",
       "      <td>36.71</td>\n",
       "      <td>53.01</td>\n",
       "      <td>6.25</td>\n",
       "      <td>49.06</td>\n",
       "      <td>54.55</td>\n",
       "      <td>38.46</td>\n",
       "      <td>55.56</td>\n",
       "      <td>61.67</td>\n",
       "      <td>35.71</td>\n",
       "      <td>21.74</td>\n",
       "      <td>47.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1_rag</th>\n",
       "      <td>60.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>65.38</td>\n",
       "      <td>69.57</td>\n",
       "      <td>77.78</td>\n",
       "      <td>46.84</td>\n",
       "      <td>70.59</td>\n",
       "      <td>18.75</td>\n",
       "      <td>65.09</td>\n",
       "      <td>72.73</td>\n",
       "      <td>61.54</td>\n",
       "      <td>55.56</td>\n",
       "      <td>74.17</td>\n",
       "      <td>42.86</td>\n",
       "      <td>39.13</td>\n",
       "      <td>61.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek</th>\n",
       "      <td>65.0</td>\n",
       "      <td>64.29</td>\n",
       "      <td>61.54</td>\n",
       "      <td>78.26</td>\n",
       "      <td>55.56</td>\n",
       "      <td>53.16</td>\n",
       "      <td>59.04</td>\n",
       "      <td>56.25</td>\n",
       "      <td>61.32</td>\n",
       "      <td>54.55</td>\n",
       "      <td>30.77</td>\n",
       "      <td>55.56</td>\n",
       "      <td>63.33</td>\n",
       "      <td>73.81</td>\n",
       "      <td>65.22</td>\n",
       "      <td>61.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_rag</th>\n",
       "      <td>80.0</td>\n",
       "      <td>78.57</td>\n",
       "      <td>76.92</td>\n",
       "      <td>91.30</td>\n",
       "      <td>66.67</td>\n",
       "      <td>65.82</td>\n",
       "      <td>63.86</td>\n",
       "      <td>68.75</td>\n",
       "      <td>70.75</td>\n",
       "      <td>72.73</td>\n",
       "      <td>46.15</td>\n",
       "      <td>66.67</td>\n",
       "      <td>72.50</td>\n",
       "      <td>83.33</td>\n",
       "      <td>73.91</td>\n",
       "      <td>71.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama</th>\n",
       "      <td>27.5</td>\n",
       "      <td>21.43</td>\n",
       "      <td>23.08</td>\n",
       "      <td>39.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.32</td>\n",
       "      <td>36.14</td>\n",
       "      <td>18.75</td>\n",
       "      <td>23.58</td>\n",
       "      <td>9.09</td>\n",
       "      <td>7.69</td>\n",
       "      <td>11.11</td>\n",
       "      <td>32.50</td>\n",
       "      <td>35.71</td>\n",
       "      <td>21.74</td>\n",
       "      <td>27.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_rag</th>\n",
       "      <td>40.0</td>\n",
       "      <td>21.43</td>\n",
       "      <td>28.85</td>\n",
       "      <td>52.17</td>\n",
       "      <td>33.33</td>\n",
       "      <td>35.44</td>\n",
       "      <td>46.99</td>\n",
       "      <td>18.75</td>\n",
       "      <td>26.42</td>\n",
       "      <td>9.09</td>\n",
       "      <td>7.69</td>\n",
       "      <td>44.44</td>\n",
       "      <td>43.33</td>\n",
       "      <td>42.86</td>\n",
       "      <td>34.78</td>\n",
       "      <td>36.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen</th>\n",
       "      <td>55.0</td>\n",
       "      <td>35.71</td>\n",
       "      <td>51.92</td>\n",
       "      <td>52.17</td>\n",
       "      <td>33.33</td>\n",
       "      <td>50.63</td>\n",
       "      <td>60.24</td>\n",
       "      <td>31.25</td>\n",
       "      <td>62.26</td>\n",
       "      <td>45.45</td>\n",
       "      <td>30.77</td>\n",
       "      <td>55.56</td>\n",
       "      <td>57.50</td>\n",
       "      <td>71.43</td>\n",
       "      <td>56.52</td>\n",
       "      <td>55.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_rag</th>\n",
       "      <td>75.0</td>\n",
       "      <td>35.71</td>\n",
       "      <td>67.31</td>\n",
       "      <td>69.57</td>\n",
       "      <td>55.56</td>\n",
       "      <td>63.29</td>\n",
       "      <td>66.27</td>\n",
       "      <td>56.25</td>\n",
       "      <td>76.42</td>\n",
       "      <td>72.73</td>\n",
       "      <td>46.15</td>\n",
       "      <td>55.56</td>\n",
       "      <td>65.00</td>\n",
       "      <td>80.95</td>\n",
       "      <td>78.26</td>\n",
       "      <td>67.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o</th>\n",
       "      <td>65.0</td>\n",
       "      <td>71.43</td>\n",
       "      <td>71.15</td>\n",
       "      <td>73.91</td>\n",
       "      <td>66.67</td>\n",
       "      <td>73.42</td>\n",
       "      <td>74.70</td>\n",
       "      <td>81.25</td>\n",
       "      <td>71.70</td>\n",
       "      <td>63.64</td>\n",
       "      <td>38.46</td>\n",
       "      <td>77.78</td>\n",
       "      <td>73.33</td>\n",
       "      <td>76.19</td>\n",
       "      <td>78.26</td>\n",
       "      <td>72.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o_rag</th>\n",
       "      <td>82.5</td>\n",
       "      <td>85.71</td>\n",
       "      <td>82.69</td>\n",
       "      <td>78.26</td>\n",
       "      <td>77.78</td>\n",
       "      <td>82.28</td>\n",
       "      <td>80.72</td>\n",
       "      <td>93.75</td>\n",
       "      <td>81.13</td>\n",
       "      <td>72.73</td>\n",
       "      <td>76.92</td>\n",
       "      <td>77.78</td>\n",
       "      <td>80.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>91.30</td>\n",
       "      <td>81.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro</th>\n",
       "      <td>82.5</td>\n",
       "      <td>50.00</td>\n",
       "      <td>69.23</td>\n",
       "      <td>69.57</td>\n",
       "      <td>55.56</td>\n",
       "      <td>64.56</td>\n",
       "      <td>64.29</td>\n",
       "      <td>81.25</td>\n",
       "      <td>74.53</td>\n",
       "      <td>63.64</td>\n",
       "      <td>53.85</td>\n",
       "      <td>88.89</td>\n",
       "      <td>72.50</td>\n",
       "      <td>83.33</td>\n",
       "      <td>69.57</td>\n",
       "      <td>70.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro_rag</th>\n",
       "      <td>85.0</td>\n",
       "      <td>71.43</td>\n",
       "      <td>82.69</td>\n",
       "      <td>86.96</td>\n",
       "      <td>66.67</td>\n",
       "      <td>72.15</td>\n",
       "      <td>77.38</td>\n",
       "      <td>93.75</td>\n",
       "      <td>88.68</td>\n",
       "      <td>72.73</td>\n",
       "      <td>84.62</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>90.48</td>\n",
       "      <td>86.96</td>\n",
       "      <td>82.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_3.5</th>\n",
       "      <td>85.0</td>\n",
       "      <td>57.14</td>\n",
       "      <td>80.77</td>\n",
       "      <td>69.57</td>\n",
       "      <td>77.78</td>\n",
       "      <td>68.35</td>\n",
       "      <td>79.52</td>\n",
       "      <td>75.00</td>\n",
       "      <td>76.42</td>\n",
       "      <td>63.64</td>\n",
       "      <td>53.85</td>\n",
       "      <td>66.67</td>\n",
       "      <td>76.67</td>\n",
       "      <td>88.10</td>\n",
       "      <td>73.91</td>\n",
       "      <td>75.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_3.5_rag</th>\n",
       "      <td>85.0</td>\n",
       "      <td>64.29</td>\n",
       "      <td>86.54</td>\n",
       "      <td>78.26</td>\n",
       "      <td>77.78</td>\n",
       "      <td>75.95</td>\n",
       "      <td>84.34</td>\n",
       "      <td>81.25</td>\n",
       "      <td>82.08</td>\n",
       "      <td>63.64</td>\n",
       "      <td>61.54</td>\n",
       "      <td>77.78</td>\n",
       "      <td>80.00</td>\n",
       "      <td>90.48</td>\n",
       "      <td>78.26</td>\n",
       "      <td>80.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl</th>\n",
       "      <td>55.0</td>\n",
       "      <td>35.71</td>\n",
       "      <td>57.69</td>\n",
       "      <td>56.52</td>\n",
       "      <td>22.22</td>\n",
       "      <td>44.30</td>\n",
       "      <td>62.65</td>\n",
       "      <td>25.00</td>\n",
       "      <td>51.89</td>\n",
       "      <td>36.36</td>\n",
       "      <td>23.08</td>\n",
       "      <td>33.33</td>\n",
       "      <td>62.50</td>\n",
       "      <td>47.62</td>\n",
       "      <td>60.87</td>\n",
       "      <td>52.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl_rag</th>\n",
       "      <td>70.0</td>\n",
       "      <td>57.14</td>\n",
       "      <td>73.08</td>\n",
       "      <td>56.52</td>\n",
       "      <td>44.44</td>\n",
       "      <td>60.76</td>\n",
       "      <td>75.90</td>\n",
       "      <td>43.75</td>\n",
       "      <td>65.09</td>\n",
       "      <td>63.64</td>\n",
       "      <td>38.46</td>\n",
       "      <td>33.33</td>\n",
       "      <td>70.00</td>\n",
       "      <td>57.14</td>\n",
       "      <td>65.22</td>\n",
       "      <td>65.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>19.23</td>\n",
       "      <td>26.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.99</td>\n",
       "      <td>21.69</td>\n",
       "      <td>12.50</td>\n",
       "      <td>16.04</td>\n",
       "      <td>9.09</td>\n",
       "      <td>7.69</td>\n",
       "      <td>11.11</td>\n",
       "      <td>20.00</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.35</td>\n",
       "      <td>16.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava_rag</th>\n",
       "      <td>25.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>34.62</td>\n",
       "      <td>26.09</td>\n",
       "      <td>22.22</td>\n",
       "      <td>29.11</td>\n",
       "      <td>38.55</td>\n",
       "      <td>12.50</td>\n",
       "      <td>31.13</td>\n",
       "      <td>27.27</td>\n",
       "      <td>30.77</td>\n",
       "      <td>11.11</td>\n",
       "      <td>27.50</td>\n",
       "      <td>14.29</td>\n",
       "      <td>30.43</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.15</td>\n",
       "      <td>43.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.53</td>\n",
       "      <td>18.07</td>\n",
       "      <td>12.50</td>\n",
       "      <td>15.09</td>\n",
       "      <td>27.27</td>\n",
       "      <td>15.38</td>\n",
       "      <td>22.22</td>\n",
       "      <td>34.17</td>\n",
       "      <td>28.57</td>\n",
       "      <td>26.09</td>\n",
       "      <td>19.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision_rag</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.77</td>\n",
       "      <td>47.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.13</td>\n",
       "      <td>24.10</td>\n",
       "      <td>31.25</td>\n",
       "      <td>23.58</td>\n",
       "      <td>36.36</td>\n",
       "      <td>38.46</td>\n",
       "      <td>33.33</td>\n",
       "      <td>43.33</td>\n",
       "      <td>38.10</td>\n",
       "      <td>30.43</td>\n",
       "      <td>27.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Investment  Quantitative Methods  \\\n",
       "claude_sonnet               52.5                 50.00   \n",
       "claude_sonnet_rag           65.0                 50.00   \n",
       "gpt_o1                      55.0                 35.71   \n",
       "gpt_o1_rag                  67.5                 50.00   \n",
       "Gemini_Pro1                 47.5                 28.57   \n",
       "Gemini_Pro1_rag             60.0                 50.00   \n",
       "deepseek                    65.0                 64.29   \n",
       "deepseek_rag                80.0                 78.57   \n",
       "llama                       27.5                 21.43   \n",
       "llama_rag                   40.0                 21.43   \n",
       "qwen                        55.0                 35.71   \n",
       "qwen_rag                    75.0                 35.71   \n",
       "gpt_4o                      65.0                 71.43   \n",
       "gpt_4o_rag                  82.5                 85.71   \n",
       "gemini_1.5_pro              82.5                 50.00   \n",
       "gemini_1.5_pro_rag          85.0                 71.43   \n",
       "claude_3.5                  85.0                 57.14   \n",
       "claude_3.5_rag              85.0                 64.29   \n",
       "qwen2_vl                    55.0                 35.71   \n",
       "qwen2_vl_rag                70.0                 57.14   \n",
       "Llava                       20.0                  7.14   \n",
       "Llava_rag                   25.0                  7.14   \n",
       "Llama_3.2_vision             5.0                  0.00   \n",
       "Llama_3.2_vision_rag         5.0                  0.00   \n",
       "\n",
       "                      Valuation and Risk Models  \\\n",
       "claude_sonnet                             65.38   \n",
       "claude_sonnet_rag                         78.85   \n",
       "gpt_o1                                    59.62   \n",
       "gpt_o1_rag                                69.23   \n",
       "Gemini_Pro1                               57.69   \n",
       "Gemini_Pro1_rag                           65.38   \n",
       "deepseek                                  61.54   \n",
       "deepseek_rag                              76.92   \n",
       "llama                                     23.08   \n",
       "llama_rag                                 28.85   \n",
       "qwen                                      51.92   \n",
       "qwen_rag                                  67.31   \n",
       "gpt_4o                                    71.15   \n",
       "gpt_4o_rag                                82.69   \n",
       "gemini_1.5_pro                            69.23   \n",
       "gemini_1.5_pro_rag                        82.69   \n",
       "claude_3.5                                80.77   \n",
       "claude_3.5_rag                            86.54   \n",
       "qwen2_vl                                  57.69   \n",
       "qwen2_vl_rag                              73.08   \n",
       "Llava                                     19.23   \n",
       "Llava_rag                                 34.62   \n",
       "Llama_3.2_vision                          21.15   \n",
       "Llama_3.2_vision_rag                      30.77   \n",
       "\n",
       "                      Financial Markets and Products  \\\n",
       "claude_sonnet                                  52.17   \n",
       "claude_sonnet_rag                              65.22   \n",
       "gpt_o1                                         56.52   \n",
       "gpt_o1_rag                                     65.22   \n",
       "Gemini_Pro1                                    60.87   \n",
       "Gemini_Pro1_rag                                69.57   \n",
       "deepseek                                       78.26   \n",
       "deepseek_rag                                   91.30   \n",
       "llama                                          39.13   \n",
       "llama_rag                                      52.17   \n",
       "qwen                                           52.17   \n",
       "qwen_rag                                       69.57   \n",
       "gpt_4o                                         73.91   \n",
       "gpt_4o_rag                                     78.26   \n",
       "gemini_1.5_pro                                 69.57   \n",
       "gemini_1.5_pro_rag                             86.96   \n",
       "claude_3.5                                     69.57   \n",
       "claude_3.5_rag                                 78.26   \n",
       "qwen2_vl                                       56.52   \n",
       "qwen2_vl_rag                                   56.52   \n",
       "Llava                                          26.09   \n",
       "Llava_rag                                      26.09   \n",
       "Llama_3.2_vision                               43.48   \n",
       "Llama_3.2_vision_rag                           47.83   \n",
       "\n",
       "                      Financial Reporting and Analysis  Portfolio Management  \\\n",
       "claude_sonnet                                    22.22                 48.10   \n",
       "claude_sonnet_rag                                22.22                 60.76   \n",
       "gpt_o1                                           33.33                 37.97   \n",
       "gpt_o1_rag                                       55.56                 49.37   \n",
       "Gemini_Pro1                                      33.33                 36.71   \n",
       "Gemini_Pro1_rag                                  77.78                 46.84   \n",
       "deepseek                                         55.56                 53.16   \n",
       "deepseek_rag                                     66.67                 65.82   \n",
       "llama                                             0.00                 25.32   \n",
       "llama_rag                                        33.33                 35.44   \n",
       "qwen                                             33.33                 50.63   \n",
       "qwen_rag                                         55.56                 63.29   \n",
       "gpt_4o                                           66.67                 73.42   \n",
       "gpt_4o_rag                                       77.78                 82.28   \n",
       "gemini_1.5_pro                                   55.56                 64.56   \n",
       "gemini_1.5_pro_rag                               66.67                 72.15   \n",
       "claude_3.5                                       77.78                 68.35   \n",
       "claude_3.5_rag                                   77.78                 75.95   \n",
       "qwen2_vl                                         22.22                 44.30   \n",
       "qwen2_vl_rag                                     44.44                 60.76   \n",
       "Llava                                             0.00                 18.99   \n",
       "Llava_rag                                        22.22                 29.11   \n",
       "Llama_3.2_vision                                  0.00                  2.53   \n",
       "Llama_3.2_vision_rag                              0.00                 10.13   \n",
       "\n",
       "                      Fixed Income  Foundation of Risk Management  \\\n",
       "claude_sonnet                60.24                          37.50   \n",
       "claude_sonnet_rag            71.08                          62.50   \n",
       "gpt_o1                       53.01                          18.75   \n",
       "gpt_o1_rag                   68.67                          31.25   \n",
       "Gemini_Pro1                  53.01                           6.25   \n",
       "Gemini_Pro1_rag              70.59                          18.75   \n",
       "deepseek                     59.04                          56.25   \n",
       "deepseek_rag                 63.86                          68.75   \n",
       "llama                        36.14                          18.75   \n",
       "llama_rag                    46.99                          18.75   \n",
       "qwen                         60.24                          31.25   \n",
       "qwen_rag                     66.27                          56.25   \n",
       "gpt_4o                       74.70                          81.25   \n",
       "gpt_4o_rag                   80.72                          93.75   \n",
       "gemini_1.5_pro               64.29                          81.25   \n",
       "gemini_1.5_pro_rag           77.38                          93.75   \n",
       "claude_3.5                   79.52                          75.00   \n",
       "claude_3.5_rag               84.34                          81.25   \n",
       "qwen2_vl                     62.65                          25.00   \n",
       "qwen2_vl_rag                 75.90                          43.75   \n",
       "Llava                        21.69                          12.50   \n",
       "Llava_rag                    38.55                          12.50   \n",
       "Llama_3.2_vision             18.07                          12.50   \n",
       "Llama_3.2_vision_rag         24.10                          31.25   \n",
       "\n",
       "                      Credit Risk  Economics  Operational Risk  Derivatives  \\\n",
       "claude_sonnet               52.83      18.18             30.77        22.22   \n",
       "claude_sonnet_rag           60.38      45.45             46.15        33.33   \n",
       "gpt_o1                      45.28      18.18             38.46        66.67   \n",
       "gpt_o1_rag                  61.32      36.36             46.15        77.78   \n",
       "Gemini_Pro1                 49.06      54.55             38.46        55.56   \n",
       "Gemini_Pro1_rag             65.09      72.73             61.54        55.56   \n",
       "deepseek                    61.32      54.55             30.77        55.56   \n",
       "deepseek_rag                70.75      72.73             46.15        66.67   \n",
       "llama                       23.58       9.09              7.69        11.11   \n",
       "llama_rag                   26.42       9.09              7.69        44.44   \n",
       "qwen                        62.26      45.45             30.77        55.56   \n",
       "qwen_rag                    76.42      72.73             46.15        55.56   \n",
       "gpt_4o                      71.70      63.64             38.46        77.78   \n",
       "gpt_4o_rag                  81.13      72.73             76.92        77.78   \n",
       "gemini_1.5_pro              74.53      63.64             53.85        88.89   \n",
       "gemini_1.5_pro_rag          88.68      72.73             84.62       100.00   \n",
       "claude_3.5                  76.42      63.64             53.85        66.67   \n",
       "claude_3.5_rag              82.08      63.64             61.54        77.78   \n",
       "qwen2_vl                    51.89      36.36             23.08        33.33   \n",
       "qwen2_vl_rag                65.09      63.64             38.46        33.33   \n",
       "Llava                       16.04       9.09              7.69        11.11   \n",
       "Llava_rag                   31.13      27.27             30.77        11.11   \n",
       "Llama_3.2_vision            15.09      27.27             15.38        22.22   \n",
       "Llama_3.2_vision_rag        23.58      36.36             38.46        33.33   \n",
       "\n",
       "                      Market Risk  Corporate Finance  \\\n",
       "claude_sonnet               65.00              47.62   \n",
       "claude_sonnet_rag           75.00              61.90   \n",
       "gpt_o1                      51.67              40.48   \n",
       "gpt_o1_rag                  65.83              54.76   \n",
       "Gemini_Pro1                 61.67              35.71   \n",
       "Gemini_Pro1_rag             74.17              42.86   \n",
       "deepseek                    63.33              73.81   \n",
       "deepseek_rag                72.50              83.33   \n",
       "llama                       32.50              35.71   \n",
       "llama_rag                   43.33              42.86   \n",
       "qwen                        57.50              71.43   \n",
       "qwen_rag                    65.00              80.95   \n",
       "gpt_4o                      73.33              76.19   \n",
       "gpt_4o_rag                  80.00              83.33   \n",
       "gemini_1.5_pro              72.50              83.33   \n",
       "gemini_1.5_pro_rag          80.00              90.48   \n",
       "claude_3.5                  76.67              88.10   \n",
       "claude_3.5_rag              80.00              90.48   \n",
       "qwen2_vl                    62.50              47.62   \n",
       "qwen2_vl_rag                70.00              57.14   \n",
       "Llava                       20.00               4.76   \n",
       "Llava_rag                   27.50              14.29   \n",
       "Llama_3.2_vision            34.17              28.57   \n",
       "Llama_3.2_vision_rag        43.33              38.10   \n",
       "\n",
       "                      Liquidity and Treasury Risk  Overall  \n",
       "claude_sonnet                               56.52    53.91  \n",
       "claude_sonnet_rag                           56.52    64.84  \n",
       "gpt_o1                                      30.43    46.56  \n",
       "gpt_o1_rag                                  47.83    60.31  \n",
       "Gemini_Pro1                                 21.74    47.81  \n",
       "Gemini_Pro1_rag                             39.13    61.37  \n",
       "deepseek                                    65.22    61.25  \n",
       "deepseek_rag                                73.91    71.88  \n",
       "llama                                       21.74    27.34  \n",
       "llama_rag                                   34.78    36.09  \n",
       "qwen                                        56.52    55.62  \n",
       "qwen_rag                                    78.26    67.97  \n",
       "gpt_4o                                      78.26    72.19  \n",
       "gpt_4o_rag                                  91.30    81.72  \n",
       "gemini_1.5_pro                              69.57    70.83  \n",
       "gemini_1.5_pro_rag                          86.96    82.06  \n",
       "claude_3.5                                  73.91    75.94  \n",
       "claude_3.5_rag                              78.26    80.78  \n",
       "qwen2_vl                                    60.87    52.66  \n",
       "qwen2_vl_rag                                65.22    65.00  \n",
       "Llava                                        4.35    16.72  \n",
       "Llava_rag                                   30.43    28.28  \n",
       "Llama_3.2_vision                            26.09    19.38  \n",
       "Llama_3.2_vision_rag                        30.43    27.19  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_Acc=[]\n",
    "for model in models:\n",
    "    topic_Acc.append(evaluate_topics(file_path(model)))\n",
    "topic_result=pd.DataFrame(topic_Acc,index=models, columns=topics)\n",
    "topic_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_result.to_csv('/Users/sden118/Desktop/FinReasoning/evaluation/Topic_Accuracy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Investment</th>\n",
       "      <th>Quantitative Methods</th>\n",
       "      <th>Valuation and Risk Models</th>\n",
       "      <th>Financial Markets and Products</th>\n",
       "      <th>Financial Reporting and Analysis</th>\n",
       "      <th>Portfolio Management</th>\n",
       "      <th>Fixed Income</th>\n",
       "      <th>Foundation of Risk Management</th>\n",
       "      <th>Credit Risk</th>\n",
       "      <th>Economics</th>\n",
       "      <th>Operational Risk</th>\n",
       "      <th>Derivatives</th>\n",
       "      <th>Market Risk</th>\n",
       "      <th>Corporate Finance</th>\n",
       "      <th>Liquidity and Treasury Risk</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claude_sonnet</th>\n",
       "      <td>52.5</td>\n",
       "      <td>50.00</td>\n",
       "      <td>65.38</td>\n",
       "      <td>52.17</td>\n",
       "      <td>22.22</td>\n",
       "      <td>48.10</td>\n",
       "      <td>60.24</td>\n",
       "      <td>37.50</td>\n",
       "      <td>52.83</td>\n",
       "      <td>18.18</td>\n",
       "      <td>30.77</td>\n",
       "      <td>22.22</td>\n",
       "      <td>65.00</td>\n",
       "      <td>47.62</td>\n",
       "      <td>56.52</td>\n",
       "      <td>53.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_sonnet_rag</th>\n",
       "      <td>65.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>78.85</td>\n",
       "      <td>65.22</td>\n",
       "      <td>22.22</td>\n",
       "      <td>60.76</td>\n",
       "      <td>71.08</td>\n",
       "      <td>62.50</td>\n",
       "      <td>60.38</td>\n",
       "      <td>45.45</td>\n",
       "      <td>46.15</td>\n",
       "      <td>33.33</td>\n",
       "      <td>75.00</td>\n",
       "      <td>61.90</td>\n",
       "      <td>56.52</td>\n",
       "      <td>64.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_o1</th>\n",
       "      <td>55.0</td>\n",
       "      <td>35.71</td>\n",
       "      <td>59.62</td>\n",
       "      <td>56.52</td>\n",
       "      <td>33.33</td>\n",
       "      <td>37.97</td>\n",
       "      <td>53.01</td>\n",
       "      <td>18.75</td>\n",
       "      <td>45.28</td>\n",
       "      <td>18.18</td>\n",
       "      <td>38.46</td>\n",
       "      <td>66.67</td>\n",
       "      <td>51.67</td>\n",
       "      <td>40.48</td>\n",
       "      <td>30.43</td>\n",
       "      <td>46.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_o1_rag</th>\n",
       "      <td>67.5</td>\n",
       "      <td>50.00</td>\n",
       "      <td>69.23</td>\n",
       "      <td>65.22</td>\n",
       "      <td>55.56</td>\n",
       "      <td>49.37</td>\n",
       "      <td>68.67</td>\n",
       "      <td>31.25</td>\n",
       "      <td>61.32</td>\n",
       "      <td>36.36</td>\n",
       "      <td>46.15</td>\n",
       "      <td>77.78</td>\n",
       "      <td>65.83</td>\n",
       "      <td>54.76</td>\n",
       "      <td>47.83</td>\n",
       "      <td>60.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1</th>\n",
       "      <td>47.5</td>\n",
       "      <td>28.57</td>\n",
       "      <td>57.69</td>\n",
       "      <td>60.87</td>\n",
       "      <td>33.33</td>\n",
       "      <td>36.71</td>\n",
       "      <td>53.01</td>\n",
       "      <td>6.25</td>\n",
       "      <td>49.06</td>\n",
       "      <td>54.55</td>\n",
       "      <td>38.46</td>\n",
       "      <td>55.56</td>\n",
       "      <td>61.67</td>\n",
       "      <td>35.71</td>\n",
       "      <td>21.74</td>\n",
       "      <td>47.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1_rag</th>\n",
       "      <td>60.0</td>\n",
       "      <td>50.00</td>\n",
       "      <td>65.38</td>\n",
       "      <td>69.57</td>\n",
       "      <td>77.78</td>\n",
       "      <td>46.84</td>\n",
       "      <td>70.59</td>\n",
       "      <td>18.75</td>\n",
       "      <td>65.09</td>\n",
       "      <td>72.73</td>\n",
       "      <td>61.54</td>\n",
       "      <td>55.56</td>\n",
       "      <td>74.17</td>\n",
       "      <td>42.86</td>\n",
       "      <td>39.13</td>\n",
       "      <td>61.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek</th>\n",
       "      <td>65.0</td>\n",
       "      <td>64.29</td>\n",
       "      <td>61.54</td>\n",
       "      <td>78.26</td>\n",
       "      <td>55.56</td>\n",
       "      <td>53.16</td>\n",
       "      <td>59.04</td>\n",
       "      <td>56.25</td>\n",
       "      <td>61.32</td>\n",
       "      <td>54.55</td>\n",
       "      <td>30.77</td>\n",
       "      <td>55.56</td>\n",
       "      <td>63.33</td>\n",
       "      <td>73.81</td>\n",
       "      <td>65.22</td>\n",
       "      <td>61.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_rag</th>\n",
       "      <td>80.0</td>\n",
       "      <td>78.57</td>\n",
       "      <td>76.92</td>\n",
       "      <td>91.30</td>\n",
       "      <td>66.67</td>\n",
       "      <td>65.82</td>\n",
       "      <td>63.86</td>\n",
       "      <td>68.75</td>\n",
       "      <td>70.75</td>\n",
       "      <td>72.73</td>\n",
       "      <td>46.15</td>\n",
       "      <td>66.67</td>\n",
       "      <td>72.50</td>\n",
       "      <td>83.33</td>\n",
       "      <td>73.91</td>\n",
       "      <td>71.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama</th>\n",
       "      <td>27.5</td>\n",
       "      <td>21.43</td>\n",
       "      <td>23.08</td>\n",
       "      <td>39.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.32</td>\n",
       "      <td>36.14</td>\n",
       "      <td>18.75</td>\n",
       "      <td>23.58</td>\n",
       "      <td>9.09</td>\n",
       "      <td>7.69</td>\n",
       "      <td>11.11</td>\n",
       "      <td>32.50</td>\n",
       "      <td>35.71</td>\n",
       "      <td>21.74</td>\n",
       "      <td>27.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_rag</th>\n",
       "      <td>40.0</td>\n",
       "      <td>21.43</td>\n",
       "      <td>28.85</td>\n",
       "      <td>52.17</td>\n",
       "      <td>33.33</td>\n",
       "      <td>35.44</td>\n",
       "      <td>46.99</td>\n",
       "      <td>18.75</td>\n",
       "      <td>26.42</td>\n",
       "      <td>9.09</td>\n",
       "      <td>7.69</td>\n",
       "      <td>44.44</td>\n",
       "      <td>43.33</td>\n",
       "      <td>42.86</td>\n",
       "      <td>34.78</td>\n",
       "      <td>36.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen</th>\n",
       "      <td>55.0</td>\n",
       "      <td>35.71</td>\n",
       "      <td>51.92</td>\n",
       "      <td>52.17</td>\n",
       "      <td>33.33</td>\n",
       "      <td>50.63</td>\n",
       "      <td>60.24</td>\n",
       "      <td>31.25</td>\n",
       "      <td>62.26</td>\n",
       "      <td>45.45</td>\n",
       "      <td>30.77</td>\n",
       "      <td>55.56</td>\n",
       "      <td>57.50</td>\n",
       "      <td>71.43</td>\n",
       "      <td>56.52</td>\n",
       "      <td>55.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_rag</th>\n",
       "      <td>75.0</td>\n",
       "      <td>35.71</td>\n",
       "      <td>67.31</td>\n",
       "      <td>69.57</td>\n",
       "      <td>55.56</td>\n",
       "      <td>63.29</td>\n",
       "      <td>66.27</td>\n",
       "      <td>56.25</td>\n",
       "      <td>76.42</td>\n",
       "      <td>72.73</td>\n",
       "      <td>46.15</td>\n",
       "      <td>55.56</td>\n",
       "      <td>65.00</td>\n",
       "      <td>80.95</td>\n",
       "      <td>78.26</td>\n",
       "      <td>67.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o</th>\n",
       "      <td>65.0</td>\n",
       "      <td>71.43</td>\n",
       "      <td>71.15</td>\n",
       "      <td>73.91</td>\n",
       "      <td>66.67</td>\n",
       "      <td>73.42</td>\n",
       "      <td>74.70</td>\n",
       "      <td>81.25</td>\n",
       "      <td>71.70</td>\n",
       "      <td>63.64</td>\n",
       "      <td>38.46</td>\n",
       "      <td>77.78</td>\n",
       "      <td>73.33</td>\n",
       "      <td>76.19</td>\n",
       "      <td>78.26</td>\n",
       "      <td>72.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o_rag</th>\n",
       "      <td>82.5</td>\n",
       "      <td>85.71</td>\n",
       "      <td>82.69</td>\n",
       "      <td>78.26</td>\n",
       "      <td>77.78</td>\n",
       "      <td>82.28</td>\n",
       "      <td>80.72</td>\n",
       "      <td>93.75</td>\n",
       "      <td>81.13</td>\n",
       "      <td>72.73</td>\n",
       "      <td>76.92</td>\n",
       "      <td>77.78</td>\n",
       "      <td>80.00</td>\n",
       "      <td>83.33</td>\n",
       "      <td>91.30</td>\n",
       "      <td>81.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro</th>\n",
       "      <td>82.5</td>\n",
       "      <td>50.00</td>\n",
       "      <td>69.23</td>\n",
       "      <td>69.57</td>\n",
       "      <td>55.56</td>\n",
       "      <td>64.56</td>\n",
       "      <td>64.29</td>\n",
       "      <td>81.25</td>\n",
       "      <td>74.53</td>\n",
       "      <td>63.64</td>\n",
       "      <td>53.85</td>\n",
       "      <td>88.89</td>\n",
       "      <td>72.50</td>\n",
       "      <td>83.33</td>\n",
       "      <td>69.57</td>\n",
       "      <td>70.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro_rag</th>\n",
       "      <td>85.0</td>\n",
       "      <td>71.43</td>\n",
       "      <td>82.69</td>\n",
       "      <td>86.96</td>\n",
       "      <td>66.67</td>\n",
       "      <td>72.15</td>\n",
       "      <td>77.38</td>\n",
       "      <td>93.75</td>\n",
       "      <td>88.68</td>\n",
       "      <td>72.73</td>\n",
       "      <td>84.62</td>\n",
       "      <td>100.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>90.48</td>\n",
       "      <td>86.96</td>\n",
       "      <td>82.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_3.5</th>\n",
       "      <td>85.0</td>\n",
       "      <td>57.14</td>\n",
       "      <td>80.77</td>\n",
       "      <td>69.57</td>\n",
       "      <td>77.78</td>\n",
       "      <td>68.35</td>\n",
       "      <td>79.52</td>\n",
       "      <td>75.00</td>\n",
       "      <td>76.42</td>\n",
       "      <td>63.64</td>\n",
       "      <td>53.85</td>\n",
       "      <td>66.67</td>\n",
       "      <td>76.67</td>\n",
       "      <td>88.10</td>\n",
       "      <td>73.91</td>\n",
       "      <td>75.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude_3.5_rag</th>\n",
       "      <td>85.0</td>\n",
       "      <td>64.29</td>\n",
       "      <td>86.54</td>\n",
       "      <td>78.26</td>\n",
       "      <td>77.78</td>\n",
       "      <td>75.95</td>\n",
       "      <td>84.34</td>\n",
       "      <td>81.25</td>\n",
       "      <td>82.08</td>\n",
       "      <td>63.64</td>\n",
       "      <td>61.54</td>\n",
       "      <td>77.78</td>\n",
       "      <td>80.00</td>\n",
       "      <td>90.48</td>\n",
       "      <td>78.26</td>\n",
       "      <td>80.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl</th>\n",
       "      <td>55.0</td>\n",
       "      <td>35.71</td>\n",
       "      <td>57.69</td>\n",
       "      <td>56.52</td>\n",
       "      <td>22.22</td>\n",
       "      <td>44.30</td>\n",
       "      <td>62.65</td>\n",
       "      <td>25.00</td>\n",
       "      <td>51.89</td>\n",
       "      <td>36.36</td>\n",
       "      <td>23.08</td>\n",
       "      <td>33.33</td>\n",
       "      <td>62.50</td>\n",
       "      <td>47.62</td>\n",
       "      <td>60.87</td>\n",
       "      <td>52.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl_rag</th>\n",
       "      <td>70.0</td>\n",
       "      <td>57.14</td>\n",
       "      <td>73.08</td>\n",
       "      <td>56.52</td>\n",
       "      <td>44.44</td>\n",
       "      <td>60.76</td>\n",
       "      <td>75.90</td>\n",
       "      <td>43.75</td>\n",
       "      <td>65.09</td>\n",
       "      <td>63.64</td>\n",
       "      <td>38.46</td>\n",
       "      <td>33.33</td>\n",
       "      <td>70.00</td>\n",
       "      <td>57.14</td>\n",
       "      <td>65.22</td>\n",
       "      <td>65.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava</th>\n",
       "      <td>20.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>19.23</td>\n",
       "      <td>26.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>18.99</td>\n",
       "      <td>21.69</td>\n",
       "      <td>12.50</td>\n",
       "      <td>16.04</td>\n",
       "      <td>9.09</td>\n",
       "      <td>7.69</td>\n",
       "      <td>11.11</td>\n",
       "      <td>20.00</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.35</td>\n",
       "      <td>16.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava_rag</th>\n",
       "      <td>25.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>34.62</td>\n",
       "      <td>26.09</td>\n",
       "      <td>22.22</td>\n",
       "      <td>29.11</td>\n",
       "      <td>38.55</td>\n",
       "      <td>12.50</td>\n",
       "      <td>31.13</td>\n",
       "      <td>27.27</td>\n",
       "      <td>30.77</td>\n",
       "      <td>11.11</td>\n",
       "      <td>27.50</td>\n",
       "      <td>14.29</td>\n",
       "      <td>30.43</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>21.15</td>\n",
       "      <td>43.48</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.53</td>\n",
       "      <td>18.07</td>\n",
       "      <td>12.50</td>\n",
       "      <td>15.09</td>\n",
       "      <td>27.27</td>\n",
       "      <td>15.38</td>\n",
       "      <td>22.22</td>\n",
       "      <td>34.17</td>\n",
       "      <td>28.57</td>\n",
       "      <td>26.09</td>\n",
       "      <td>19.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision_rag</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30.77</td>\n",
       "      <td>47.83</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.13</td>\n",
       "      <td>24.10</td>\n",
       "      <td>31.25</td>\n",
       "      <td>23.58</td>\n",
       "      <td>36.36</td>\n",
       "      <td>38.46</td>\n",
       "      <td>33.33</td>\n",
       "      <td>43.33</td>\n",
       "      <td>38.10</td>\n",
       "      <td>30.43</td>\n",
       "      <td>27.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Investment  Quantitative Methods  \\\n",
       "claude_sonnet               52.5                 50.00   \n",
       "claude_sonnet_rag           65.0                 50.00   \n",
       "gpt_o1                      55.0                 35.71   \n",
       "gpt_o1_rag                  67.5                 50.00   \n",
       "Gemini_Pro1                 47.5                 28.57   \n",
       "Gemini_Pro1_rag             60.0                 50.00   \n",
       "deepseek                    65.0                 64.29   \n",
       "deepseek_rag                80.0                 78.57   \n",
       "llama                       27.5                 21.43   \n",
       "llama_rag                   40.0                 21.43   \n",
       "qwen                        55.0                 35.71   \n",
       "qwen_rag                    75.0                 35.71   \n",
       "gpt_4o                      65.0                 71.43   \n",
       "gpt_4o_rag                  82.5                 85.71   \n",
       "gemini_1.5_pro              82.5                 50.00   \n",
       "gemini_1.5_pro_rag          85.0                 71.43   \n",
       "claude_3.5                  85.0                 57.14   \n",
       "claude_3.5_rag              85.0                 64.29   \n",
       "qwen2_vl                    55.0                 35.71   \n",
       "qwen2_vl_rag                70.0                 57.14   \n",
       "Llava                       20.0                  7.14   \n",
       "Llava_rag                   25.0                  7.14   \n",
       "Llama_3.2_vision             5.0                  0.00   \n",
       "Llama_3.2_vision_rag         5.0                  0.00   \n",
       "\n",
       "                      Valuation and Risk Models  \\\n",
       "claude_sonnet                             65.38   \n",
       "claude_sonnet_rag                         78.85   \n",
       "gpt_o1                                    59.62   \n",
       "gpt_o1_rag                                69.23   \n",
       "Gemini_Pro1                               57.69   \n",
       "Gemini_Pro1_rag                           65.38   \n",
       "deepseek                                  61.54   \n",
       "deepseek_rag                              76.92   \n",
       "llama                                     23.08   \n",
       "llama_rag                                 28.85   \n",
       "qwen                                      51.92   \n",
       "qwen_rag                                  67.31   \n",
       "gpt_4o                                    71.15   \n",
       "gpt_4o_rag                                82.69   \n",
       "gemini_1.5_pro                            69.23   \n",
       "gemini_1.5_pro_rag                        82.69   \n",
       "claude_3.5                                80.77   \n",
       "claude_3.5_rag                            86.54   \n",
       "qwen2_vl                                  57.69   \n",
       "qwen2_vl_rag                              73.08   \n",
       "Llava                                     19.23   \n",
       "Llava_rag                                 34.62   \n",
       "Llama_3.2_vision                          21.15   \n",
       "Llama_3.2_vision_rag                      30.77   \n",
       "\n",
       "                      Financial Markets and Products  \\\n",
       "claude_sonnet                                  52.17   \n",
       "claude_sonnet_rag                              65.22   \n",
       "gpt_o1                                         56.52   \n",
       "gpt_o1_rag                                     65.22   \n",
       "Gemini_Pro1                                    60.87   \n",
       "Gemini_Pro1_rag                                69.57   \n",
       "deepseek                                       78.26   \n",
       "deepseek_rag                                   91.30   \n",
       "llama                                          39.13   \n",
       "llama_rag                                      52.17   \n",
       "qwen                                           52.17   \n",
       "qwen_rag                                       69.57   \n",
       "gpt_4o                                         73.91   \n",
       "gpt_4o_rag                                     78.26   \n",
       "gemini_1.5_pro                                 69.57   \n",
       "gemini_1.5_pro_rag                             86.96   \n",
       "claude_3.5                                     69.57   \n",
       "claude_3.5_rag                                 78.26   \n",
       "qwen2_vl                                       56.52   \n",
       "qwen2_vl_rag                                   56.52   \n",
       "Llava                                          26.09   \n",
       "Llava_rag                                      26.09   \n",
       "Llama_3.2_vision                               43.48   \n",
       "Llama_3.2_vision_rag                           47.83   \n",
       "\n",
       "                      Financial Reporting and Analysis  Portfolio Management  \\\n",
       "claude_sonnet                                    22.22                 48.10   \n",
       "claude_sonnet_rag                                22.22                 60.76   \n",
       "gpt_o1                                           33.33                 37.97   \n",
       "gpt_o1_rag                                       55.56                 49.37   \n",
       "Gemini_Pro1                                      33.33                 36.71   \n",
       "Gemini_Pro1_rag                                  77.78                 46.84   \n",
       "deepseek                                         55.56                 53.16   \n",
       "deepseek_rag                                     66.67                 65.82   \n",
       "llama                                             0.00                 25.32   \n",
       "llama_rag                                        33.33                 35.44   \n",
       "qwen                                             33.33                 50.63   \n",
       "qwen_rag                                         55.56                 63.29   \n",
       "gpt_4o                                           66.67                 73.42   \n",
       "gpt_4o_rag                                       77.78                 82.28   \n",
       "gemini_1.5_pro                                   55.56                 64.56   \n",
       "gemini_1.5_pro_rag                               66.67                 72.15   \n",
       "claude_3.5                                       77.78                 68.35   \n",
       "claude_3.5_rag                                   77.78                 75.95   \n",
       "qwen2_vl                                         22.22                 44.30   \n",
       "qwen2_vl_rag                                     44.44                 60.76   \n",
       "Llava                                             0.00                 18.99   \n",
       "Llava_rag                                        22.22                 29.11   \n",
       "Llama_3.2_vision                                  0.00                  2.53   \n",
       "Llama_3.2_vision_rag                              0.00                 10.13   \n",
       "\n",
       "                      Fixed Income  Foundation of Risk Management  \\\n",
       "claude_sonnet                60.24                          37.50   \n",
       "claude_sonnet_rag            71.08                          62.50   \n",
       "gpt_o1                       53.01                          18.75   \n",
       "gpt_o1_rag                   68.67                          31.25   \n",
       "Gemini_Pro1                  53.01                           6.25   \n",
       "Gemini_Pro1_rag              70.59                          18.75   \n",
       "deepseek                     59.04                          56.25   \n",
       "deepseek_rag                 63.86                          68.75   \n",
       "llama                        36.14                          18.75   \n",
       "llama_rag                    46.99                          18.75   \n",
       "qwen                         60.24                          31.25   \n",
       "qwen_rag                     66.27                          56.25   \n",
       "gpt_4o                       74.70                          81.25   \n",
       "gpt_4o_rag                   80.72                          93.75   \n",
       "gemini_1.5_pro               64.29                          81.25   \n",
       "gemini_1.5_pro_rag           77.38                          93.75   \n",
       "claude_3.5                   79.52                          75.00   \n",
       "claude_3.5_rag               84.34                          81.25   \n",
       "qwen2_vl                     62.65                          25.00   \n",
       "qwen2_vl_rag                 75.90                          43.75   \n",
       "Llava                        21.69                          12.50   \n",
       "Llava_rag                    38.55                          12.50   \n",
       "Llama_3.2_vision             18.07                          12.50   \n",
       "Llama_3.2_vision_rag         24.10                          31.25   \n",
       "\n",
       "                      Credit Risk  Economics  Operational Risk  Derivatives  \\\n",
       "claude_sonnet               52.83      18.18             30.77        22.22   \n",
       "claude_sonnet_rag           60.38      45.45             46.15        33.33   \n",
       "gpt_o1                      45.28      18.18             38.46        66.67   \n",
       "gpt_o1_rag                  61.32      36.36             46.15        77.78   \n",
       "Gemini_Pro1                 49.06      54.55             38.46        55.56   \n",
       "Gemini_Pro1_rag             65.09      72.73             61.54        55.56   \n",
       "deepseek                    61.32      54.55             30.77        55.56   \n",
       "deepseek_rag                70.75      72.73             46.15        66.67   \n",
       "llama                       23.58       9.09              7.69        11.11   \n",
       "llama_rag                   26.42       9.09              7.69        44.44   \n",
       "qwen                        62.26      45.45             30.77        55.56   \n",
       "qwen_rag                    76.42      72.73             46.15        55.56   \n",
       "gpt_4o                      71.70      63.64             38.46        77.78   \n",
       "gpt_4o_rag                  81.13      72.73             76.92        77.78   \n",
       "gemini_1.5_pro              74.53      63.64             53.85        88.89   \n",
       "gemini_1.5_pro_rag          88.68      72.73             84.62       100.00   \n",
       "claude_3.5                  76.42      63.64             53.85        66.67   \n",
       "claude_3.5_rag              82.08      63.64             61.54        77.78   \n",
       "qwen2_vl                    51.89      36.36             23.08        33.33   \n",
       "qwen2_vl_rag                65.09      63.64             38.46        33.33   \n",
       "Llava                       16.04       9.09              7.69        11.11   \n",
       "Llava_rag                   31.13      27.27             30.77        11.11   \n",
       "Llama_3.2_vision            15.09      27.27             15.38        22.22   \n",
       "Llama_3.2_vision_rag        23.58      36.36             38.46        33.33   \n",
       "\n",
       "                      Market Risk  Corporate Finance  \\\n",
       "claude_sonnet               65.00              47.62   \n",
       "claude_sonnet_rag           75.00              61.90   \n",
       "gpt_o1                      51.67              40.48   \n",
       "gpt_o1_rag                  65.83              54.76   \n",
       "Gemini_Pro1                 61.67              35.71   \n",
       "Gemini_Pro1_rag             74.17              42.86   \n",
       "deepseek                    63.33              73.81   \n",
       "deepseek_rag                72.50              83.33   \n",
       "llama                       32.50              35.71   \n",
       "llama_rag                   43.33              42.86   \n",
       "qwen                        57.50              71.43   \n",
       "qwen_rag                    65.00              80.95   \n",
       "gpt_4o                      73.33              76.19   \n",
       "gpt_4o_rag                  80.00              83.33   \n",
       "gemini_1.5_pro              72.50              83.33   \n",
       "gemini_1.5_pro_rag          80.00              90.48   \n",
       "claude_3.5                  76.67              88.10   \n",
       "claude_3.5_rag              80.00              90.48   \n",
       "qwen2_vl                    62.50              47.62   \n",
       "qwen2_vl_rag                70.00              57.14   \n",
       "Llava                       20.00               4.76   \n",
       "Llava_rag                   27.50              14.29   \n",
       "Llama_3.2_vision            34.17              28.57   \n",
       "Llama_3.2_vision_rag        43.33              38.10   \n",
       "\n",
       "                      Liquidity and Treasury Risk  Overall  \n",
       "claude_sonnet                               56.52    53.91  \n",
       "claude_sonnet_rag                           56.52    64.84  \n",
       "gpt_o1                                      30.43    46.56  \n",
       "gpt_o1_rag                                  47.83    60.31  \n",
       "Gemini_Pro1                                 21.74    47.81  \n",
       "Gemini_Pro1_rag                             39.13    61.37  \n",
       "deepseek                                    65.22    61.25  \n",
       "deepseek_rag                                73.91    71.88  \n",
       "llama                                       21.74    27.34  \n",
       "llama_rag                                   34.78    36.09  \n",
       "qwen                                        56.52    55.62  \n",
       "qwen_rag                                    78.26    67.97  \n",
       "gpt_4o                                      78.26    72.19  \n",
       "gpt_4o_rag                                  91.30    81.72  \n",
       "gemini_1.5_pro                              69.57    70.83  \n",
       "gemini_1.5_pro_rag                          86.96    82.06  \n",
       "claude_3.5                                  73.91    75.94  \n",
       "claude_3.5_rag                              78.26    80.78  \n",
       "qwen2_vl                                    60.87    52.66  \n",
       "qwen2_vl_rag                                65.22    65.00  \n",
       "Llava                                        4.35    16.72  \n",
       "Llava_rag                                   30.43    28.28  \n",
       "Llama_3.2_vision                            26.09    19.38  \n",
       "Llama_3.2_vision_rag                        30.43    27.19  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_Acc=[]\n",
    "for model in models:\n",
    "    topic_Acc.append(evaluate_topics_test(file_path(model)))\n",
    "topic_result_test=pd.DataFrame(topic_Acc,index=models, columns=topics)\n",
    "topic_result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_result_test.to_csv('/Users/sden118/Desktop/FinReasoning/evaluation/Topic_Accuracy_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>error type</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>calculation</th>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image</th>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>option</th>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>2315</td>\n",
       "      <td>2315</td>\n",
       "      <td>2315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  error type  Model\n",
       "Error                               \n",
       "calculation   276         276    276\n",
       "image         102         102    102\n",
       "option        147         147    147\n",
       "other         338         338    338\n",
       "question     2315        2315   2315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/sden118/Desktop/FinReasoning/evaluation/errorType.csv')\n",
    "data.groupby(['Error']).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
