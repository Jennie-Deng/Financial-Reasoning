{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_path (model):\n",
    "    filePath=f\"/Volumes/Jennie/Reasoning/FinReasoning/output/{model}_output.json\"\n",
    "    return filePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    # a=0\n",
    "    # for question in data:\n",
    "    #     if question[\"Model Answer\"] == question[\"Answer\"]:\n",
    "    #         a+=1\n",
    "    #     else:\n",
    "    #         a+=0\n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "    a=data[\"isCorrect\"].sum()\n",
    "    b=data['isCorrect'].count()\n",
    "    accuracy=(a/b*100).__round__(2)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_topics(eva_file_path):\n",
    "    data=pd.read_json(eva_file_path)\n",
    "    # 计算每个答案是否正确\n",
    "    data[\"isCorrect\"] = data.apply(lambda row: 1 if row['Model Answer'] == row['Answer'] else 0, axis=1)\n",
    "\n",
    "    data['General Topics']=data['General Topics'].apply(lambda x: 'Others' if x in other_topics  else x)\n",
    "\n",
    "    # 计算每个 General Topics 的正确答案数量\n",
    "    correct_by_topic = data.groupby('General Topics')['isCorrect'].sum().reset_index()\n",
    "\n",
    "    # 计算每个 General Topics 的总答案数量\n",
    "    count_by_topic = data.groupby('General Topics')['isCorrect'].count().reset_index()\n",
    "\n",
    "    # 合并两个 DataFrame\n",
    "    accuracy_by_topic = pd.merge(correct_by_topic, count_by_topic, on='General Topics')\n",
    "\n",
    "    # 计算正确率\n",
    "    accuracy_by_topic['accuracy%'] = round(accuracy_by_topic['isCorrect_x'] / accuracy_by_topic['isCorrect_y'] * 100, 2)\n",
    "\n",
    "    # 重命名列\n",
    "    accuracy_by_topic.columns = ['General Topics', 'correct', 'total', 'accuracy%']\n",
    "\n",
    "    # 计算总体正确率\n",
    "    total_correct = data['isCorrect'].sum()\n",
    "    total_count = data['isCorrect'].count()\n",
    "    overall_accuracy = round((total_correct / total_count) * 100, 2)\n",
    "\n",
    "    # 创建一个新的 DataFrame 来存储总体正确率\n",
    "    overall_accuracy_df = pd.DataFrame([['Overall', total_correct, total_count, overall_accuracy]], columns=['General Topics', 'correct', 'total', 'accuracy%'])\n",
    "\n",
    "    # 将总体正确率 DataFrame 与 accuracy_by_topic 进行合并\n",
    "    final_df = pd.concat([accuracy_by_topic, overall_accuracy_df], ignore_index=True)\n",
    "\n",
    "    # 打印结果\n",
    "    accuracy=list(final_df['accuracy%'])\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[\n",
    "        \"gpt_o1\",\"gpt_o1_rag\",\n",
    "        \"Gemini_Pro1\",\"Gemini_Pro1_rag\",\n",
    "        \"deepseek\",\"deepseek_rag\",\n",
    "        \"llama\",\"llama_rag\",\n",
    "        \"gpt_4o\",\"gpt_4o_rag\",\n",
    "        \"gemini_1.5_pro\",\"gemini_1.5_pro_rag\",\n",
    "        \"qwen\",\"qwen_rag\",\n",
    "        \"Llava\",\"Llava_rag\",\n",
    "        \"Llama_3.2_vision\",\"Llama_3.2_vision_rag\",\n",
    "        \"qwen2_vl\",\"qwen2_vl_rag\",\n",
    "        ]\n",
    "topics=['Alternative Investments',\n",
    "#  'Asset Allocation',\n",
    "#  'Capital Market Expectations',\n",
    " 'Corporate Finance',\n",
    " 'Credit Risk Measurement and Management',\n",
    " 'Derivatives',\n",
    "#  'Derivatives and Currency Management',\n",
    " 'Economics',\n",
    " 'Equity Investments',\n",
    "#  'Equity Portfolio Management',\n",
    "#  'Ethicaland Professional Standards',\n",
    " 'Financial Markets and Products',\n",
    " 'Financial Reporting and Analysis',\n",
    " 'Fixed Income',\n",
    "#  'Fixed-Income Portfolio Management',\n",
    " 'Foundation of Risk Management',\n",
    " 'Liquidity and Treasury Risk Measurement',\n",
    " 'Market Risk Measurement and Management',\n",
    " 'Operational Risk',\n",
    "#  'Operational Risk and Resiliency',\n",
    " 'Portfolio Management',\n",
    " 'Quantitative Methods',\n",
    " 'Risk Management and Investment Management',\n",
    " 'Valuation and Risk Models',\n",
    " 'Overall']\n",
    "other_topics=['Fixed-Income Portfolio Management','Operational Risk','Equity Portfolio Management','Derivatives and Currency Management',\n",
    "              'Asset Allocation','Ethicaland Professional Standards','Capital Market Expectations']\n",
    "column=['Alternative Investments',\n",
    "#  'Asset Allocation',\n",
    "#  'Capital Market Expectations',\n",
    " 'Corporate Finance',\n",
    " 'Credit Risk Measurement and Management',\n",
    " 'Derivatives',\n",
    "#  'Derivatives and Currency Management',\n",
    " 'Economics',\n",
    " 'Equity Investments',\n",
    "#  'Equity Portfolio Management',\n",
    "#  'Ethicaland Professional Standards',\n",
    " 'Financial Markets and Products',\n",
    " 'Financial Reporting and Analysis',\n",
    " 'Fixed Income',\n",
    "#  'Fixed-Income Portfolio Management',\n",
    " 'Foundation of Risk Management',\n",
    " 'Liquidity and Treasury Risk Measurement',\n",
    " 'Market Risk Measurement and Management',\n",
    " 'Operational Risk',\n",
    "#  'Operational Risk and Resiliency',\n",
    " 'Portfolio Management',\n",
    " 'Quantitative Methods',\n",
    " 'Risk Management and Investment Management',\n",
    " 'Valuation and Risk Models',\n",
    " \"Others\",\n",
    " 'Overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_o1</td>\n",
       "      <td>67.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_o1_rag</td>\n",
       "      <td>80.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini_Pro1</td>\n",
       "      <td>70.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemini_Pro1_rag</td>\n",
       "      <td>78.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek_rag</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama_rag</td>\n",
       "      <td>47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>79.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt_4o_rag</td>\n",
       "      <td>85.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemini_1.5_pro_rag</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qwen</td>\n",
       "      <td>66.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qwen_rag</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Llava</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Llava_rag</td>\n",
       "      <td>35.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Llama_3.2_vision</td>\n",
       "      <td>22.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama_3.2_vision_rag</td>\n",
       "      <td>36.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>qwen2_vl</td>\n",
       "      <td>73.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>qwen2_vl_rag</td>\n",
       "      <td>80.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy%\n",
       "0                 gpt_o1      67.67\n",
       "1             gpt_o1_rag      80.41\n",
       "2            Gemini_Pro1      70.39\n",
       "3        Gemini_Pro1_rag      78.68\n",
       "4               deepseek      70.90\n",
       "5           deepseek_rag      78.75\n",
       "6                  llama      35.20\n",
       "7              llama_rag      47.95\n",
       "8                 gpt_4o      79.69\n",
       "9             gpt_4o_rag      85.28\n",
       "10        gemini_1.5_pro      77.53\n",
       "11    gemini_1.5_pro_rag      86.00\n",
       "12                  qwen      66.02\n",
       "13              qwen_rag      76.68\n",
       "14                 Llava      28.73\n",
       "15             Llava_rag      35.47\n",
       "16      Llama_3.2_vision      22.06\n",
       "17  Llama_3.2_vision_rag      36.88\n",
       "18              qwen2_vl      73.27\n",
       "19          qwen2_vl_rag      80.40"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelAcc=[]\n",
    "for model in models:\n",
    "    modelAcc.append(evaluate(file_path(model)))\n",
    "result={\n",
    "    'Model': models,\n",
    "    'Accuracy%': modelAcc\n",
    "}\n",
    "\n",
    "result=pd.DataFrame(result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/Volumes/Jennie/Reasoning/FinReasoning/evaluation/Model_Accuracy.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gemini_1.5_pro_rag</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpt_4o_rag</td>\n",
       "      <td>85.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt_o1_rag</td>\n",
       "      <td>80.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpt_4o</td>\n",
       "      <td>79.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek_rag</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemini_Pro1_rag</td>\n",
       "      <td>78.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gemini_1.5_pro</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qwen_rag</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemini_Pro1</td>\n",
       "      <td>70.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt_o1</td>\n",
       "      <td>67.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>qwen</td>\n",
       "      <td>66.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama_rag</td>\n",
       "      <td>47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama_3.2_vision_rag</td>\n",
       "      <td>36.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Llava_rag</td>\n",
       "      <td>35.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Llava</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Llama_3.2_vision</td>\n",
       "      <td>22.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy%\n",
       "11    gemini_1.5_pro_rag      86.00\n",
       "9             gpt_4o_rag      85.28\n",
       "1             gpt_o1_rag      80.41\n",
       "8                 gpt_4o      79.69\n",
       "5           deepseek_rag      78.75\n",
       "3        Gemini_Pro1_rag      78.68\n",
       "10        gemini_1.5_pro      77.53\n",
       "13              qwen_rag      76.68\n",
       "4               deepseek      70.90\n",
       "2            Gemini_Pro1      70.39\n",
       "0                 gpt_o1      67.67\n",
       "12                  qwen      66.02\n",
       "7              llama_rag      47.95\n",
       "17  Llama_3.2_vision_rag      36.88\n",
       "15             Llava_rag      35.47\n",
       "6                  llama      35.20\n",
       "14                 Llava      28.73\n",
       "16      Llama_3.2_vision      22.06"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values(by='Accuracy%', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alternative Investments</th>\n",
       "      <th>Corporate Finance</th>\n",
       "      <th>Credit Risk Measurement and Management</th>\n",
       "      <th>Derivatives</th>\n",
       "      <th>Economics</th>\n",
       "      <th>Equity Investments</th>\n",
       "      <th>Financial Markets and Products</th>\n",
       "      <th>Financial Reporting and Analysis</th>\n",
       "      <th>Fixed Income</th>\n",
       "      <th>Foundation of Risk Management</th>\n",
       "      <th>Liquidity and Treasury Risk Measurement</th>\n",
       "      <th>Market Risk Measurement and Management</th>\n",
       "      <th>Operational Risk</th>\n",
       "      <th>Portfolio Management</th>\n",
       "      <th>Quantitative Methods</th>\n",
       "      <th>Risk Management and Investment Management</th>\n",
       "      <th>Valuation and Risk Models</th>\n",
       "      <th>Others</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt_o1</th>\n",
       "      <td>67.50</td>\n",
       "      <td>67.71</td>\n",
       "      <td>71.07</td>\n",
       "      <td>65.98</td>\n",
       "      <td>74.68</td>\n",
       "      <td>63.58</td>\n",
       "      <td>74.16</td>\n",
       "      <td>67.07</td>\n",
       "      <td>78.49</td>\n",
       "      <td>63.23</td>\n",
       "      <td>61.76</td>\n",
       "      <td>63.72</td>\n",
       "      <td>57.14</td>\n",
       "      <td>51.10</td>\n",
       "      <td>65.79</td>\n",
       "      <td>77.26</td>\n",
       "      <td>55.66</td>\n",
       "      <td>67.13</td>\n",
       "      <td>67.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_o1_rag</th>\n",
       "      <td>77.50</td>\n",
       "      <td>83.33</td>\n",
       "      <td>81.76</td>\n",
       "      <td>79.38</td>\n",
       "      <td>85.99</td>\n",
       "      <td>77.78</td>\n",
       "      <td>87.64</td>\n",
       "      <td>77.33</td>\n",
       "      <td>88.71</td>\n",
       "      <td>75.97</td>\n",
       "      <td>76.24</td>\n",
       "      <td>74.34</td>\n",
       "      <td>70.33</td>\n",
       "      <td>65.38</td>\n",
       "      <td>80.42</td>\n",
       "      <td>86.92</td>\n",
       "      <td>70.75</td>\n",
       "      <td>84.08</td>\n",
       "      <td>80.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1</th>\n",
       "      <td>76.25</td>\n",
       "      <td>65.62</td>\n",
       "      <td>49.37</td>\n",
       "      <td>80.41</td>\n",
       "      <td>81.65</td>\n",
       "      <td>76.54</td>\n",
       "      <td>76.32</td>\n",
       "      <td>71.43</td>\n",
       "      <td>59.12</td>\n",
       "      <td>66.45</td>\n",
       "      <td>67.65</td>\n",
       "      <td>64.60</td>\n",
       "      <td>71.43</td>\n",
       "      <td>58.01</td>\n",
       "      <td>76.84</td>\n",
       "      <td>81.62</td>\n",
       "      <td>60.38</td>\n",
       "      <td>68.51</td>\n",
       "      <td>70.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gemini_Pro1_rag</th>\n",
       "      <td>81.25</td>\n",
       "      <td>78.12</td>\n",
       "      <td>62.42</td>\n",
       "      <td>83.51</td>\n",
       "      <td>84.81</td>\n",
       "      <td>84.57</td>\n",
       "      <td>84.91</td>\n",
       "      <td>78.78</td>\n",
       "      <td>69.36</td>\n",
       "      <td>72.90</td>\n",
       "      <td>80.39</td>\n",
       "      <td>72.57</td>\n",
       "      <td>81.32</td>\n",
       "      <td>70.98</td>\n",
       "      <td>83.68</td>\n",
       "      <td>85.67</td>\n",
       "      <td>70.75</td>\n",
       "      <td>79.58</td>\n",
       "      <td>78.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek</th>\n",
       "      <td>73.75</td>\n",
       "      <td>69.79</td>\n",
       "      <td>58.49</td>\n",
       "      <td>73.20</td>\n",
       "      <td>80.38</td>\n",
       "      <td>76.54</td>\n",
       "      <td>72.66</td>\n",
       "      <td>73.17</td>\n",
       "      <td>77.96</td>\n",
       "      <td>69.68</td>\n",
       "      <td>53.92</td>\n",
       "      <td>55.75</td>\n",
       "      <td>64.84</td>\n",
       "      <td>54.40</td>\n",
       "      <td>78.95</td>\n",
       "      <td>82.87</td>\n",
       "      <td>63.21</td>\n",
       "      <td>69.20</td>\n",
       "      <td>70.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deepseek_rag</th>\n",
       "      <td>80.00</td>\n",
       "      <td>80.21</td>\n",
       "      <td>65.41</td>\n",
       "      <td>82.47</td>\n",
       "      <td>88.61</td>\n",
       "      <td>82.10</td>\n",
       "      <td>81.65</td>\n",
       "      <td>80.08</td>\n",
       "      <td>83.33</td>\n",
       "      <td>74.19</td>\n",
       "      <td>70.59</td>\n",
       "      <td>63.72</td>\n",
       "      <td>71.43</td>\n",
       "      <td>63.89</td>\n",
       "      <td>85.79</td>\n",
       "      <td>88.16</td>\n",
       "      <td>73.58</td>\n",
       "      <td>79.58</td>\n",
       "      <td>78.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama</th>\n",
       "      <td>36.25</td>\n",
       "      <td>37.50</td>\n",
       "      <td>49.69</td>\n",
       "      <td>26.80</td>\n",
       "      <td>43.04</td>\n",
       "      <td>38.89</td>\n",
       "      <td>25.09</td>\n",
       "      <td>35.37</td>\n",
       "      <td>67.20</td>\n",
       "      <td>32.90</td>\n",
       "      <td>24.51</td>\n",
       "      <td>19.47</td>\n",
       "      <td>37.36</td>\n",
       "      <td>26.92</td>\n",
       "      <td>31.58</td>\n",
       "      <td>40.19</td>\n",
       "      <td>26.42</td>\n",
       "      <td>26.99</td>\n",
       "      <td>35.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_rag</th>\n",
       "      <td>44.87</td>\n",
       "      <td>46.88</td>\n",
       "      <td>59.87</td>\n",
       "      <td>41.24</td>\n",
       "      <td>58.86</td>\n",
       "      <td>49.38</td>\n",
       "      <td>40.47</td>\n",
       "      <td>46.34</td>\n",
       "      <td>77.96</td>\n",
       "      <td>48.37</td>\n",
       "      <td>42.00</td>\n",
       "      <td>35.40</td>\n",
       "      <td>50.55</td>\n",
       "      <td>35.71</td>\n",
       "      <td>47.37</td>\n",
       "      <td>52.34</td>\n",
       "      <td>40.00</td>\n",
       "      <td>38.49</td>\n",
       "      <td>47.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o</th>\n",
       "      <td>76.25</td>\n",
       "      <td>75.79</td>\n",
       "      <td>72.90</td>\n",
       "      <td>85.57</td>\n",
       "      <td>89.03</td>\n",
       "      <td>86.42</td>\n",
       "      <td>81.58</td>\n",
       "      <td>85.31</td>\n",
       "      <td>88.52</td>\n",
       "      <td>84.40</td>\n",
       "      <td>74.49</td>\n",
       "      <td>63.39</td>\n",
       "      <td>69.66</td>\n",
       "      <td>63.22</td>\n",
       "      <td>85.26</td>\n",
       "      <td>85.05</td>\n",
       "      <td>70.87</td>\n",
       "      <td>75.00</td>\n",
       "      <td>79.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt_4o_rag</th>\n",
       "      <td>88.75</td>\n",
       "      <td>87.37</td>\n",
       "      <td>81.29</td>\n",
       "      <td>89.69</td>\n",
       "      <td>90.32</td>\n",
       "      <td>88.89</td>\n",
       "      <td>87.97</td>\n",
       "      <td>89.80</td>\n",
       "      <td>91.80</td>\n",
       "      <td>88.73</td>\n",
       "      <td>82.65</td>\n",
       "      <td>71.43</td>\n",
       "      <td>73.03</td>\n",
       "      <td>74.14</td>\n",
       "      <td>87.89</td>\n",
       "      <td>87.54</td>\n",
       "      <td>80.58</td>\n",
       "      <td>81.60</td>\n",
       "      <td>85.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro</th>\n",
       "      <td>81.25</td>\n",
       "      <td>83.33</td>\n",
       "      <td>66.67</td>\n",
       "      <td>80.41</td>\n",
       "      <td>86.71</td>\n",
       "      <td>83.33</td>\n",
       "      <td>75.66</td>\n",
       "      <td>79.67</td>\n",
       "      <td>82.26</td>\n",
       "      <td>84.52</td>\n",
       "      <td>69.61</td>\n",
       "      <td>65.49</td>\n",
       "      <td>71.43</td>\n",
       "      <td>59.34</td>\n",
       "      <td>82.63</td>\n",
       "      <td>89.10</td>\n",
       "      <td>73.58</td>\n",
       "      <td>70.59</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini_1.5_pro_rag</th>\n",
       "      <td>90.00</td>\n",
       "      <td>90.62</td>\n",
       "      <td>74.84</td>\n",
       "      <td>89.69</td>\n",
       "      <td>92.41</td>\n",
       "      <td>92.59</td>\n",
       "      <td>84.27</td>\n",
       "      <td>86.59</td>\n",
       "      <td>89.78</td>\n",
       "      <td>88.39</td>\n",
       "      <td>81.37</td>\n",
       "      <td>76.99</td>\n",
       "      <td>76.92</td>\n",
       "      <td>76.37</td>\n",
       "      <td>87.37</td>\n",
       "      <td>92.83</td>\n",
       "      <td>80.19</td>\n",
       "      <td>86.16</td>\n",
       "      <td>86.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen</th>\n",
       "      <td>71.25</td>\n",
       "      <td>56.25</td>\n",
       "      <td>53.46</td>\n",
       "      <td>62.89</td>\n",
       "      <td>77.85</td>\n",
       "      <td>75.31</td>\n",
       "      <td>64.04</td>\n",
       "      <td>67.48</td>\n",
       "      <td>69.89</td>\n",
       "      <td>68.39</td>\n",
       "      <td>52.94</td>\n",
       "      <td>40.71</td>\n",
       "      <td>67.03</td>\n",
       "      <td>50.00</td>\n",
       "      <td>72.11</td>\n",
       "      <td>82.35</td>\n",
       "      <td>65.09</td>\n",
       "      <td>63.32</td>\n",
       "      <td>66.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen_rag</th>\n",
       "      <td>85.00</td>\n",
       "      <td>71.88</td>\n",
       "      <td>67.30</td>\n",
       "      <td>74.23</td>\n",
       "      <td>84.81</td>\n",
       "      <td>83.33</td>\n",
       "      <td>74.53</td>\n",
       "      <td>78.05</td>\n",
       "      <td>79.03</td>\n",
       "      <td>78.06</td>\n",
       "      <td>67.65</td>\n",
       "      <td>61.95</td>\n",
       "      <td>78.02</td>\n",
       "      <td>60.44</td>\n",
       "      <td>78.95</td>\n",
       "      <td>88.85</td>\n",
       "      <td>75.47</td>\n",
       "      <td>76.47</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava</th>\n",
       "      <td>26.25</td>\n",
       "      <td>25.00</td>\n",
       "      <td>22.64</td>\n",
       "      <td>35.05</td>\n",
       "      <td>35.44</td>\n",
       "      <td>34.57</td>\n",
       "      <td>26.59</td>\n",
       "      <td>36.59</td>\n",
       "      <td>38.71</td>\n",
       "      <td>24.52</td>\n",
       "      <td>17.65</td>\n",
       "      <td>23.01</td>\n",
       "      <td>35.16</td>\n",
       "      <td>17.03</td>\n",
       "      <td>34.74</td>\n",
       "      <td>28.97</td>\n",
       "      <td>25.47</td>\n",
       "      <td>24.57</td>\n",
       "      <td>28.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llava_rag</th>\n",
       "      <td>38.75</td>\n",
       "      <td>35.42</td>\n",
       "      <td>28.30</td>\n",
       "      <td>45.36</td>\n",
       "      <td>39.87</td>\n",
       "      <td>43.21</td>\n",
       "      <td>31.84</td>\n",
       "      <td>43.90</td>\n",
       "      <td>47.31</td>\n",
       "      <td>30.32</td>\n",
       "      <td>25.49</td>\n",
       "      <td>30.97</td>\n",
       "      <td>39.56</td>\n",
       "      <td>29.12</td>\n",
       "      <td>41.58</td>\n",
       "      <td>31.78</td>\n",
       "      <td>31.13</td>\n",
       "      <td>29.41</td>\n",
       "      <td>35.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision</th>\n",
       "      <td>33.33</td>\n",
       "      <td>16.67</td>\n",
       "      <td>20.13</td>\n",
       "      <td>15.46</td>\n",
       "      <td>31.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>16.85</td>\n",
       "      <td>26.42</td>\n",
       "      <td>26.88</td>\n",
       "      <td>18.33</td>\n",
       "      <td>14.47</td>\n",
       "      <td>30.43</td>\n",
       "      <td>20.75</td>\n",
       "      <td>22.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama_3.2_vision_rag</th>\n",
       "      <td>60.61</td>\n",
       "      <td>26.04</td>\n",
       "      <td>37.11</td>\n",
       "      <td>30.93</td>\n",
       "      <td>51.16</td>\n",
       "      <td>3.33</td>\n",
       "      <td>32.58</td>\n",
       "      <td>41.87</td>\n",
       "      <td>39.25</td>\n",
       "      <td>31.67</td>\n",
       "      <td>25.00</td>\n",
       "      <td>49.57</td>\n",
       "      <td>29.25</td>\n",
       "      <td>36.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl</th>\n",
       "      <td>73.75</td>\n",
       "      <td>64.58</td>\n",
       "      <td>63.52</td>\n",
       "      <td>76.29</td>\n",
       "      <td>84.81</td>\n",
       "      <td>74.07</td>\n",
       "      <td>74.53</td>\n",
       "      <td>74.39</td>\n",
       "      <td>80.11</td>\n",
       "      <td>78.06</td>\n",
       "      <td>69.61</td>\n",
       "      <td>58.41</td>\n",
       "      <td>74.73</td>\n",
       "      <td>53.85</td>\n",
       "      <td>80.00</td>\n",
       "      <td>82.24</td>\n",
       "      <td>66.98</td>\n",
       "      <td>71.28</td>\n",
       "      <td>73.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen2_vl_rag</th>\n",
       "      <td>82.50</td>\n",
       "      <td>76.04</td>\n",
       "      <td>71.70</td>\n",
       "      <td>83.51</td>\n",
       "      <td>85.44</td>\n",
       "      <td>82.10</td>\n",
       "      <td>80.90</td>\n",
       "      <td>81.30</td>\n",
       "      <td>87.10</td>\n",
       "      <td>83.87</td>\n",
       "      <td>79.41</td>\n",
       "      <td>68.14</td>\n",
       "      <td>79.12</td>\n",
       "      <td>66.48</td>\n",
       "      <td>83.68</td>\n",
       "      <td>86.29</td>\n",
       "      <td>76.42</td>\n",
       "      <td>80.97</td>\n",
       "      <td>80.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Alternative Investments  Corporate Finance  \\\n",
       "gpt_o1                                  67.50              67.71   \n",
       "gpt_o1_rag                              77.50              83.33   \n",
       "Gemini_Pro1                             76.25              65.62   \n",
       "Gemini_Pro1_rag                         81.25              78.12   \n",
       "deepseek                                73.75              69.79   \n",
       "deepseek_rag                            80.00              80.21   \n",
       "llama                                   36.25              37.50   \n",
       "llama_rag                               44.87              46.88   \n",
       "gpt_4o                                  76.25              75.79   \n",
       "gpt_4o_rag                              88.75              87.37   \n",
       "gemini_1.5_pro                          81.25              83.33   \n",
       "gemini_1.5_pro_rag                      90.00              90.62   \n",
       "qwen                                    71.25              56.25   \n",
       "qwen_rag                                85.00              71.88   \n",
       "Llava                                   26.25              25.00   \n",
       "Llava_rag                               38.75              35.42   \n",
       "Llama_3.2_vision                        33.33              16.67   \n",
       "Llama_3.2_vision_rag                    60.61              26.04   \n",
       "qwen2_vl                                73.75              64.58   \n",
       "qwen2_vl_rag                            82.50              76.04   \n",
       "\n",
       "                      Credit Risk Measurement and Management  Derivatives  \\\n",
       "gpt_o1                                                 71.07        65.98   \n",
       "gpt_o1_rag                                             81.76        79.38   \n",
       "Gemini_Pro1                                            49.37        80.41   \n",
       "Gemini_Pro1_rag                                        62.42        83.51   \n",
       "deepseek                                               58.49        73.20   \n",
       "deepseek_rag                                           65.41        82.47   \n",
       "llama                                                  49.69        26.80   \n",
       "llama_rag                                              59.87        41.24   \n",
       "gpt_4o                                                 72.90        85.57   \n",
       "gpt_4o_rag                                             81.29        89.69   \n",
       "gemini_1.5_pro                                         66.67        80.41   \n",
       "gemini_1.5_pro_rag                                     74.84        89.69   \n",
       "qwen                                                   53.46        62.89   \n",
       "qwen_rag                                               67.30        74.23   \n",
       "Llava                                                  22.64        35.05   \n",
       "Llava_rag                                              28.30        45.36   \n",
       "Llama_3.2_vision                                       20.13        15.46   \n",
       "Llama_3.2_vision_rag                                   37.11        30.93   \n",
       "qwen2_vl                                               63.52        76.29   \n",
       "qwen2_vl_rag                                           71.70        83.51   \n",
       "\n",
       "                      Economics  Equity Investments  \\\n",
       "gpt_o1                    74.68               63.58   \n",
       "gpt_o1_rag                85.99               77.78   \n",
       "Gemini_Pro1               81.65               76.54   \n",
       "Gemini_Pro1_rag           84.81               84.57   \n",
       "deepseek                  80.38               76.54   \n",
       "deepseek_rag              88.61               82.10   \n",
       "llama                     43.04               38.89   \n",
       "llama_rag                 58.86               49.38   \n",
       "gpt_4o                    89.03               86.42   \n",
       "gpt_4o_rag                90.32               88.89   \n",
       "gemini_1.5_pro            86.71               83.33   \n",
       "gemini_1.5_pro_rag        92.41               92.59   \n",
       "qwen                      77.85               75.31   \n",
       "qwen_rag                  84.81               83.33   \n",
       "Llava                     35.44               34.57   \n",
       "Llava_rag                 39.87               43.21   \n",
       "Llama_3.2_vision          31.01                0.00   \n",
       "Llama_3.2_vision_rag      51.16                3.33   \n",
       "qwen2_vl                  84.81               74.07   \n",
       "qwen2_vl_rag              85.44               82.10   \n",
       "\n",
       "                      Financial Markets and Products  \\\n",
       "gpt_o1                                         74.16   \n",
       "gpt_o1_rag                                     87.64   \n",
       "Gemini_Pro1                                    76.32   \n",
       "Gemini_Pro1_rag                                84.91   \n",
       "deepseek                                       72.66   \n",
       "deepseek_rag                                   81.65   \n",
       "llama                                          25.09   \n",
       "llama_rag                                      40.47   \n",
       "gpt_4o                                         81.58   \n",
       "gpt_4o_rag                                     87.97   \n",
       "gemini_1.5_pro                                 75.66   \n",
       "gemini_1.5_pro_rag                             84.27   \n",
       "qwen                                           64.04   \n",
       "qwen_rag                                       74.53   \n",
       "Llava                                          26.59   \n",
       "Llava_rag                                      31.84   \n",
       "Llama_3.2_vision                               16.85   \n",
       "Llama_3.2_vision_rag                           32.58   \n",
       "qwen2_vl                                       74.53   \n",
       "qwen2_vl_rag                                   80.90   \n",
       "\n",
       "                      Financial Reporting and Analysis  Fixed Income  \\\n",
       "gpt_o1                                           67.07         78.49   \n",
       "gpt_o1_rag                                       77.33         88.71   \n",
       "Gemini_Pro1                                      71.43         59.12   \n",
       "Gemini_Pro1_rag                                  78.78         69.36   \n",
       "deepseek                                         73.17         77.96   \n",
       "deepseek_rag                                     80.08         83.33   \n",
       "llama                                            35.37         67.20   \n",
       "llama_rag                                        46.34         77.96   \n",
       "gpt_4o                                           85.31         88.52   \n",
       "gpt_4o_rag                                       89.80         91.80   \n",
       "gemini_1.5_pro                                   79.67         82.26   \n",
       "gemini_1.5_pro_rag                               86.59         89.78   \n",
       "qwen                                             67.48         69.89   \n",
       "qwen_rag                                         78.05         79.03   \n",
       "Llava                                            36.59         38.71   \n",
       "Llava_rag                                        43.90         47.31   \n",
       "Llama_3.2_vision                                 26.42         26.88   \n",
       "Llama_3.2_vision_rag                             41.87         39.25   \n",
       "qwen2_vl                                         74.39         80.11   \n",
       "qwen2_vl_rag                                     81.30         87.10   \n",
       "\n",
       "                      Foundation of Risk Management  \\\n",
       "gpt_o1                                        63.23   \n",
       "gpt_o1_rag                                    75.97   \n",
       "Gemini_Pro1                                   66.45   \n",
       "Gemini_Pro1_rag                               72.90   \n",
       "deepseek                                      69.68   \n",
       "deepseek_rag                                  74.19   \n",
       "llama                                         32.90   \n",
       "llama_rag                                     48.37   \n",
       "gpt_4o                                        84.40   \n",
       "gpt_4o_rag                                    88.73   \n",
       "gemini_1.5_pro                                84.52   \n",
       "gemini_1.5_pro_rag                            88.39   \n",
       "qwen                                          68.39   \n",
       "qwen_rag                                      78.06   \n",
       "Llava                                         24.52   \n",
       "Llava_rag                                     30.32   \n",
       "Llama_3.2_vision                              18.33   \n",
       "Llama_3.2_vision_rag                          31.67   \n",
       "qwen2_vl                                      78.06   \n",
       "qwen2_vl_rag                                  83.87   \n",
       "\n",
       "                      Liquidity and Treasury Risk Measurement  \\\n",
       "gpt_o1                                                  61.76   \n",
       "gpt_o1_rag                                              76.24   \n",
       "Gemini_Pro1                                             67.65   \n",
       "Gemini_Pro1_rag                                         80.39   \n",
       "deepseek                                                53.92   \n",
       "deepseek_rag                                            70.59   \n",
       "llama                                                   24.51   \n",
       "llama_rag                                               42.00   \n",
       "gpt_4o                                                  74.49   \n",
       "gpt_4o_rag                                              82.65   \n",
       "gemini_1.5_pro                                          69.61   \n",
       "gemini_1.5_pro_rag                                      81.37   \n",
       "qwen                                                    52.94   \n",
       "qwen_rag                                                67.65   \n",
       "Llava                                                   17.65   \n",
       "Llava_rag                                               25.49   \n",
       "Llama_3.2_vision                                        14.47   \n",
       "Llama_3.2_vision_rag                                    25.00   \n",
       "qwen2_vl                                                69.61   \n",
       "qwen2_vl_rag                                            79.41   \n",
       "\n",
       "                      Market Risk Measurement and Management  \\\n",
       "gpt_o1                                                 63.72   \n",
       "gpt_o1_rag                                             74.34   \n",
       "Gemini_Pro1                                            64.60   \n",
       "Gemini_Pro1_rag                                        72.57   \n",
       "deepseek                                               55.75   \n",
       "deepseek_rag                                           63.72   \n",
       "llama                                                  19.47   \n",
       "llama_rag                                              35.40   \n",
       "gpt_4o                                                 63.39   \n",
       "gpt_4o_rag                                             71.43   \n",
       "gemini_1.5_pro                                         65.49   \n",
       "gemini_1.5_pro_rag                                     76.99   \n",
       "qwen                                                   40.71   \n",
       "qwen_rag                                               61.95   \n",
       "Llava                                                  23.01   \n",
       "Llava_rag                                              30.97   \n",
       "Llama_3.2_vision                                       30.43   \n",
       "Llama_3.2_vision_rag                                   49.57   \n",
       "qwen2_vl                                               58.41   \n",
       "qwen2_vl_rag                                           68.14   \n",
       "\n",
       "                      Operational Risk  Portfolio Management  \\\n",
       "gpt_o1                           57.14                 51.10   \n",
       "gpt_o1_rag                       70.33                 65.38   \n",
       "Gemini_Pro1                      71.43                 58.01   \n",
       "Gemini_Pro1_rag                  81.32                 70.98   \n",
       "deepseek                         64.84                 54.40   \n",
       "deepseek_rag                     71.43                 63.89   \n",
       "llama                            37.36                 26.92   \n",
       "llama_rag                        50.55                 35.71   \n",
       "gpt_4o                           69.66                 63.22   \n",
       "gpt_4o_rag                       73.03                 74.14   \n",
       "gemini_1.5_pro                   71.43                 59.34   \n",
       "gemini_1.5_pro_rag               76.92                 76.37   \n",
       "qwen                             67.03                 50.00   \n",
       "qwen_rag                         78.02                 60.44   \n",
       "Llava                            35.16                 17.03   \n",
       "Llava_rag                        39.56                 29.12   \n",
       "Llama_3.2_vision                 20.75                 22.06   \n",
       "Llama_3.2_vision_rag             29.25                 36.88   \n",
       "qwen2_vl                         74.73                 53.85   \n",
       "qwen2_vl_rag                     79.12                 66.48   \n",
       "\n",
       "                      Quantitative Methods  \\\n",
       "gpt_o1                               65.79   \n",
       "gpt_o1_rag                           80.42   \n",
       "Gemini_Pro1                          76.84   \n",
       "Gemini_Pro1_rag                      83.68   \n",
       "deepseek                             78.95   \n",
       "deepseek_rag                         85.79   \n",
       "llama                                31.58   \n",
       "llama_rag                            47.37   \n",
       "gpt_4o                               85.26   \n",
       "gpt_4o_rag                           87.89   \n",
       "gemini_1.5_pro                       82.63   \n",
       "gemini_1.5_pro_rag                   87.37   \n",
       "qwen                                 72.11   \n",
       "qwen_rag                             78.95   \n",
       "Llava                                34.74   \n",
       "Llava_rag                            41.58   \n",
       "Llama_3.2_vision                       NaN   \n",
       "Llama_3.2_vision_rag                   NaN   \n",
       "qwen2_vl                             80.00   \n",
       "qwen2_vl_rag                         83.68   \n",
       "\n",
       "                      Risk Management and Investment Management  \\\n",
       "gpt_o1                                                    77.26   \n",
       "gpt_o1_rag                                                86.92   \n",
       "Gemini_Pro1                                               81.62   \n",
       "Gemini_Pro1_rag                                           85.67   \n",
       "deepseek                                                  82.87   \n",
       "deepseek_rag                                              88.16   \n",
       "llama                                                     40.19   \n",
       "llama_rag                                                 52.34   \n",
       "gpt_4o                                                    85.05   \n",
       "gpt_4o_rag                                                87.54   \n",
       "gemini_1.5_pro                                            89.10   \n",
       "gemini_1.5_pro_rag                                        92.83   \n",
       "qwen                                                      82.35   \n",
       "qwen_rag                                                  88.85   \n",
       "Llava                                                     28.97   \n",
       "Llava_rag                                                 31.78   \n",
       "Llama_3.2_vision                                            NaN   \n",
       "Llama_3.2_vision_rag                                        NaN   \n",
       "qwen2_vl                                                  82.24   \n",
       "qwen2_vl_rag                                              86.29   \n",
       "\n",
       "                      Valuation and Risk Models  Others  Overall  \n",
       "gpt_o1                                    55.66   67.13    67.67  \n",
       "gpt_o1_rag                                70.75   84.08    80.41  \n",
       "Gemini_Pro1                               60.38   68.51    70.39  \n",
       "Gemini_Pro1_rag                           70.75   79.58    78.68  \n",
       "deepseek                                  63.21   69.20    70.90  \n",
       "deepseek_rag                              73.58   79.58    78.75  \n",
       "llama                                     26.42   26.99    35.20  \n",
       "llama_rag                                 40.00   38.49    47.95  \n",
       "gpt_4o                                    70.87   75.00    79.69  \n",
       "gpt_4o_rag                                80.58   81.60    85.28  \n",
       "gemini_1.5_pro                            73.58   70.59    77.53  \n",
       "gemini_1.5_pro_rag                        80.19   86.16    86.00  \n",
       "qwen                                      65.09   63.32    66.02  \n",
       "qwen_rag                                  75.47   76.47    76.68  \n",
       "Llava                                     25.47   24.57    28.73  \n",
       "Llava_rag                                 31.13   29.41    35.47  \n",
       "Llama_3.2_vision                            NaN     NaN      NaN  \n",
       "Llama_3.2_vision_rag                        NaN     NaN      NaN  \n",
       "qwen2_vl                                  66.98   71.28    73.27  \n",
       "qwen2_vl_rag                              76.42   80.97    80.40  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_Acc=[]\n",
    "for model in models:\n",
    "    topic_Acc.append(evaluate_topics(file_path(model)))\n",
    "topic_result=pd.DataFrame(topic_Acc,index=models, columns=column)\n",
    "topic_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_result.to_csv('/Volumes/Jennie/Reasoning/FinReasoning/evaluation/Topic_Accuracy1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>error type</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Error</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>calculation</th>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image</th>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>option</th>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>2315</td>\n",
       "      <td>2315</td>\n",
       "      <td>2315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID  error type  Model\n",
       "Error                               \n",
       "calculation   276         276    276\n",
       "image         102         102    102\n",
       "option        147         147    147\n",
       "other         338         338    338\n",
       "question     2315        2315   2315"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('/Users/sden118/Desktop/FinReasoning/evaluation/errorType.csv')\n",
    "data.groupby(['Error']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
