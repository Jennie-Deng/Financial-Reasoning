{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import base64\n",
    "import re\n",
    "from IPython.display import display, Markdown,Image\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# 获取环境变量\n",
    "langchain_tracing = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
    "langchain_endpoint = os.getenv(\"LANGCHAIN_ENDPOINT\")\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "langchain_project = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "api_key=os.getenv(\"API_KEY\")\n",
    "base_url=os.getenv(\"BASE_URL\")\n",
    "deepseek_api_key=os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "deepseek_base_url=os.getenv(\"DEEPSEEK_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputPrompt (question):\n",
    "       \n",
    "    # 构建系统消息\n",
    "    system_message = SystemMessage(\n",
    "        content=\"You are a financial expert. You will be given questions and options, possibly with context information and images. Please answer the question.\"\n",
    "    )\n",
    "\n",
    "    # 构建用户消息\n",
    "    human_message=HumanMessage(content=[])\n",
    "\n",
    "    if len(question[\"Share Context\"]) != 0:\n",
    "        human_message.content.append({\"type\": \"text\", \"text\": \"Context: \" + question[\"Share Context\"]})\n",
    "\n",
    "    if len(question[\"Share Image\"])!= 0:\n",
    "        for path in question[\"Share Image\"]:\n",
    "            image_url = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+path\n",
    "            with open(image_url, \"rb\") as image_file:\n",
    "                image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "            human_message.content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Question: \"+ question[\"Question Text\"]})\n",
    "\n",
    "    if len(question[\"Image\"]) != 0:\n",
    "        image_url = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+question[\"Image\"]\n",
    "        with open(image_url, \"rb\") as image_file:\n",
    "            image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        human_message.content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Options: \" + str(question[\"Options\"])})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Let's think step by step. The output reasoning steps are in Markdown format. Finally, must put the correct option (A, B, C, or D) in【 】. e.g.Therefore, the correct option is 【B】.\"})\n",
    "\n",
    "    response = [system_message, human_message]\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "##Utils\n",
    "def FeedbackPrompt (question):\n",
    "       \n",
    "    system_message = SystemMessage(\n",
    "            content=\"\"\"You are a financial expert. You will be given questions and options, possibly with context information and images. Also, you will be given wrong reasoning steps and correct reasoning hints.You are supposed to give feedback.\"\"\")\n",
    "\n",
    "    # 构建用户消息\n",
    "    human_message=HumanMessage(content=[])\n",
    "\n",
    "    if len(question[\"Share Context\"]) != 0:\n",
    "        human_message.content.append({\"type\": \"text\", \"text\": \"Context: \" + question[\"Share Context\"]})\n",
    "\n",
    "    if len(question[\"Share Image\"])!= 0:\n",
    "        for path in question[\"Share Image\"]:\n",
    "            image_url = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+path\n",
    "            with open(image_url, \"rb\") as image_file:\n",
    "                image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "            human_message.content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Question: \"+ question[\"Question Text\"]})\n",
    "\n",
    "    if len(question[\"Image\"]) != 0:\n",
    "        image_url = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+question[\"Image\"]\n",
    "        with open(image_url, \"rb\") as image_file:\n",
    "            image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        human_message.content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Options: \" + str(question[\"Options\"])})\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Wrong Reasoning Steps: \" + question[\"Model Reasoning\"]})\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Wrong Answer: \" + question[\"Model Answer\"]})\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Correct Reasoning Steps: \" + question[\"Explanation\"]})\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Correct Answer: \" + question[\"Answer\"]})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"\"\" Please give the feedback in Markdown format. 1. Please output correct reasoning steps according to hints. 2. compare the correct reasoning step with the model's wrong reasoning step, and point out the difference. 3. summarize the hint for future simalar questions.\"\"\"})\n",
    "\n",
    "    response = [system_message, human_message]\n",
    "    return response\n",
    "\n",
    "\n",
    "def ICLPrompt (question,example):\n",
    "       \n",
    "    # 构建系统消息\n",
    "    system_message = SystemMessage(\n",
    "        content=\"You are a financial expert. You will be given previous learning document including questions and options, possibly with context information and images. Please answer the current question.\"\n",
    "    )\n",
    "\n",
    "    # 构建用户消息\n",
    "    human_message=HumanMessage(content=[])\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Previous Learning Document: \"})\n",
    "    if len(example[\"Share Context\"]) != 0:\n",
    "        human_message.content.append({\"type\": \"text\", \"text\": \"Context: \" + example[\"Share Context\"]})\n",
    "\n",
    "    if len(example[\"Share Image\"])!= 0:\n",
    "        for path in example[\"Share Image\"]:\n",
    "            image_url = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+path\n",
    "            with open(image_url, \"rb\") as image_file:\n",
    "                image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "            human_message.content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Question: \"+ example[\"Question Text\"]})\n",
    "\n",
    "    if len(example[\"Image\"]) != 0:\n",
    "        image_url = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+example[\"Image\"]\n",
    "        with open(image_url, \"rb\") as image_file:\n",
    "            image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        human_message.content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Options: \" + str(example[\"Options\"])})\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Wrong Reasoning Steps: \" + example[\"Model Reasoning\"]})\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Feedback: \" + example[\"Feedback\"]})\n",
    "\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Current Question is as follows: \"})\n",
    "    if len(question[\"Share Context\"]) != 0:\n",
    "        human_message.content.append({\"type\": \"text\", \"text\": \"Context: \" + question[\"Share Context\"]})\n",
    "\n",
    "    if len(question[\"Share Image\"])!= 0:\n",
    "        for path in question[\"Share Image\"]:\n",
    "            image_url = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+path\n",
    "            with open(image_url, \"rb\") as image_file:\n",
    "                image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "            human_message.content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Question: \"+ question[\"Question Text\"]})\n",
    "\n",
    "    if len(question[\"Image\"]) != 0:\n",
    "        image_url = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+question[\"Image\"]\n",
    "        with open(image_url, \"rb\") as image_file:\n",
    "            image_data = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "        human_message.content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"}})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Options: \" + str(question[\"Options\"])})\n",
    "\n",
    "    human_message.content.append({\"type\": \"text\", \"text\": \"Let's think step by step. The output reasoning steps are in Markdown format. Finally, must put the correct option (A, B, C, or D) in【 】. e.g.Therefore, the correct option is 【B】.\"})\n",
    "\n",
    "    response = [system_message, human_message]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import BaseGenerationOutputParser\n",
    "from langchain_core.outputs import Generation\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "class MarkdownParser(BaseGenerationOutputParser[str]):\n",
    "    \"\"\"\n",
    "    A custom parser that formats the model output for Markdown display\n",
    "    by replacing LaTeX-style delimiters \\[ and \\] with $.\n",
    "    \"\"\"\n",
    "    def parse_result(self, result: List[Generation], *, partial: bool = False) -> str:\n",
    "        \"\"\"Parse the model output and format it as Markdown.\n",
    "\n",
    "        Args:\n",
    "            result: A list of Generations (assumed to contain only one string).\n",
    "            partial: Whether to allow partial results (for streaming, not used here).\n",
    "\n",
    "        Returns:\n",
    "            A Markdown-formatted string with LaTeX-style delimiters replaced.\n",
    "        \"\"\"\n",
    "        # Ensure there's only one generation\n",
    "        if len(result) != 1:\n",
    "            raise ValueError(\"This parser only supports a single generation.\")\n",
    "        \n",
    "        # Extract the generation content\n",
    "        generation = result[0]\n",
    "        if not isinstance(generation.text, str):\n",
    "            raise ValueError(\"Expected text output for Markdown formatting.\")\n",
    "        \n",
    "        # Replace  \\\\[ and \\\\] with $ for LaTeX-style display\n",
    "        formatted_text = generation.text.replace('\\\\[', '$').replace('\\\\]', '$').replace('\\\\(', '$').replace('\\\\)', '$')\n",
    "        \n",
    "        import re\n",
    "\n",
    "\n",
    "\n",
    "        return formatted_text\n",
    "    \n",
    "\n",
    "import re\n",
    "from langchain.tools import Tool\n",
    "\n",
    "def extract_answer(text: str) -> str:\n",
    "    \"\"\"Extract the answer option (A, B, C, or D) in brackets from the given text.\"\"\"\n",
    "    # Regular expression to find the answer in brackets, e.g., [C]\n",
    "    match = re.search(r\"\\【([A-D])\\】\", text)\n",
    "    if match:\n",
    "        return match.group(1)  # Returns the answer option (e.g., \"C\")\n",
    "    else:\n",
    "        return \"Answer not found\"  # Returns a message if no answer is found\n",
    "\n",
    "# Wrap extract_answer in a LangChain Tool to make it invokable\n",
    "extract_answer_tool = Tool.from_function(\n",
    "    func=extract_answer,\n",
    "    name=\"Extract Answer Tool\",\n",
    "    description=\"Extracts the answer option in brackets (e.g., 【C】) from the provided text.\"\n",
    ")\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def write_output(data, file_path):\n",
    "    # 如果文件存在，先读取现有数据\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            existing_data = json.load(f)\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    # 合并新数据到现有数据中\n",
    "    if isinstance(existing_data, list):\n",
    "        existing_data.append(data)\n",
    "    else:\n",
    "        existing_data = data\n",
    "\n",
    "    # 将合并后的数据写入到 JSON 文件\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(existing_data, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "file_path = '/Volumes/Jennie/Reasoning/FinMath/dataset/testdata.json'\n",
    "data = json.loads(Path(file_path).read_text())\n",
    "da=data[0]\n",
    "write_output(da, ModelRAGOutputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "gptmodel = ChatOpenAI(model=\"gpt-4o\", api_key=os.getenv(\"API_KEY\"), base_url=os.getenv(\"BASE_URL\"))\n",
    "outputParser=MarkdownParser()\n",
    "chain = gptmodel|outputParser\n",
    "\n",
    "ErrorLogPath=\"/Volumes/Jennie/Reasoning/FinMath/errorLog/gpt_4o_ErrorLog.json\"\n",
    "ModelOutputPath=\"/Volumes/Jennie/Reasoning/FinMath/output/gpt_4o_output.json\"\n",
    "ModelRAGOutputPath=\"/Volumes/Jennie/Reasoning/FinMath/output/gpt_4o_rag_output.json\"\n",
    "\n",
    "index=init_faiss()# 初始化数据库\n",
    "index_faiss(index,ErrorLogPath) #把现在的errorlog加入到向量数据库\n",
    "\n",
    "for question in data:\n",
    "    reasoning=chain.invoke(inputPrompt(question))\n",
    "    answer=extract_answer(reasoning)    \n",
    "\n",
    "    modelOutput=question\n",
    "    modelOutput[\"Model Answer\"]=answer\n",
    "    modelOutput[\"Model Reasoning\"]=reasoning\n",
    "    write_output(modelOutput, ModelOutputPath)\n",
    "\n",
    "    if answer == question[\"Answer\"]:\n",
    "        write_output(modelOutput, ModelRAGOutputPath)\n",
    "    else:\n",
    "        cos,I = query_embedding_faiss(question, index, k=5)\n",
    "        erroRLog=json.loads(Path(ErrorLogPath).read_text())\n",
    "        errorexample=erroRLog[I[0][0]]\n",
    "        reasoning=chain.invoke(ICLPrompt(question,errorexample))\n",
    "        answer=extract_answer(reasoning)\n",
    "        display(Markdown(reasoning))\n",
    "        print(\"=====================================\")\n",
    "        modelOutput[\"Model Answer\"]=answer\n",
    "        modelOutput[\"Model Reasoning\"]=reasoning  \n",
    "        write_output(modelOutput, ModelRAGOutputPath) \n",
    "\n",
    "        feedback=chain.invoke(FeedbackPrompt(modelOutput))\n",
    "        modelOutput[\"Feedback\"]=feedback\n",
    "        write_output(modelOutput, ErrorLogPath)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the JSON data from ModelOutputPath\n",
    "model_output_path = Path(ModelOutputPath)\n",
    "data1 = json.loads(model_output_path.read_text())\n",
    "df1 = pd.DataFrame(data1)\n",
    "model_output_path = Path(ModelRAGOutputPath)\n",
    "data2 = json.loads(model_output_path.read_text())\n",
    "df2 = pd.DataFrame(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(eva_file_path):\n",
    "    data=json.loads(Path(eva_file_path).read_text())\n",
    "    a=0\n",
    "    for question in data:\n",
    "        if question[\"Model Answer\"] == question[\"Answer\"]:\n",
    "            a+=1\n",
    "        else:\n",
    "            a+=0\n",
    "    accuracy=a/len(data)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "data1 = evaluate(ModelOutputPath)\n",
    "data2 = evaluate(ModelRAGOutputPath)\n",
    "print(data1,data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Markdown,Image,Latex, display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import faiss\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载CLIP模型和处理器\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "#初始化数据库\n",
    "def init_faiss():\n",
    "    index = faiss.IndexFlatIP(1024)  # 使用内积度量\n",
    "    if index.ntotal==0:\n",
    "        # 初始化 Faiss 索引，使用内积 (dot product) 作为距离度量\n",
    "        pass\n",
    "    # 清空Faiss索引中的所有向量\n",
    "    else:\n",
    "        index.reset()\n",
    "    # 检查索引是否清空\n",
    "    print(\"Number of vectors after reset:\", index.ntotal)\n",
    "    return index\n",
    "def index_faiss(index, file_path):\n",
    "    # 检查文件是否存在\n",
    "    if os.path.exists(file_path):\n",
    "        # 文件存在，加载数据\n",
    "        data = json.loads(Path(file_path).read_text())\n",
    "    else:\n",
    "        # 文件不存在，创建一个新的空列表\n",
    "        data =     [{\"ID\": 9999,\"Question Number\": 9999,\"Share Context\": \"\",\"Share Image\": \"\",\"Question Text\": \"text\",\"Image\": \"images/QuantitativeAnalysis1_images/40u.png\",\n",
    "                     \"Options\": {\"A\": \" -0.215\",\"B\": \" -0.113\",\"C\": \" 0.113\",\"D\": \" 0.215\"},\"Answer\": \"C\",\"Explanation\": \"text\",\"QA Type\": \"text\",\"Question Type\": \"text\",\"Level of Difficulty\": \"text\",\n",
    "                    \"Knowledge Topics\": \"text\",\"General Topics\":\"text\",\"Book Label\": \"text\",\"Model Answer\": \"C\",\"Model Reasoning\": \"text\",\"Feedback\": \"text\"\n",
    "                    }]\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "    # 处理数据\n",
    "    for error in data:\n",
    "        storeEmbedding(index, error)\n",
    "\n",
    "    print(\"Number of vectors after adding:\", index.ntotal)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipEmbedding(data):\n",
    "    textdata = \"Question:\" + data.get(\"Question Text\") + \" Options:\" + str(data.get(\"Options\")) + \" Correct Answer:\" + data.get(\"Answer\")\n",
    "    \n",
    "    # 检查是否有图片\n",
    "    if data.get(\"Image\") != '':\n",
    "        image_path = \"/Volumes/Jennie/Reasoning/FinMath/dataset/\"+ data.get(\"Image\")\n",
    "        # print(image_path)\n",
    "        image = Image.open(image_path)\n",
    "        # print(image)\n",
    "        \n",
    "        # 生成文本和图像的嵌入，添加 truncation=True 和 max_length=77\n",
    "        inputs = processor(text=[textdata], images=image, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "        \n",
    "        # 使用CLIP模型生成嵌入\n",
    "        outputs = model(**inputs)\n",
    "        image_embedding = outputs.image_embeds  # 图像嵌入\n",
    "        text_embedding = outputs.text_embeds  # 文本嵌入\n",
    "    else:\n",
    "        # 如果没有图像，生成文本嵌入\n",
    "        inputs = processor(text=[textdata], return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "        text_embedding = model.get_text_features(**inputs)\n",
    "        \n",
    "        # 创建一个与图像嵌入维度相同的零向量\n",
    "        image_embedding = torch.zeros((text_embedding.shape[0], 512))  # 假设图像嵌入维度是512\n",
    "    \n",
    "    # 将文本和图像嵌入拼接在一起\n",
    "    combined_embedding = torch.cat((text_embedding, image_embedding), dim=-1)\n",
    "    \n",
    "    return combined_embedding\n",
    "\n",
    "\n",
    "def normalize(embeddings):\n",
    "    # 归一化函数，计算余弦相似度时将向量进行归一化\n",
    "    norms = torch.norm(embeddings, dim=1, keepdim=True)  # 计算每个向量的范数\n",
    "    return embeddings / norms  # 将向量归一化，使其范数变为1\n",
    "\n",
    "\n",
    "# 生成嵌入\n",
    "def storeEmbedding(index,data):\n",
    "    error_log_embedding = clipEmbedding(data)\n",
    "    # 对嵌入进行归一化，以便计算余弦相似度\n",
    "    error_log_embedding = normalize(error_log_embedding)\n",
    "    # 将生成的多模态嵌入转换为numpy数组并添加到Faiss索引中\n",
    "    error_log_embedding_np = error_log_embedding.detach().numpy()  # 确保转换为numpy格式\n",
    "    index.add(error_log_embedding_np)  # 将嵌入添加到Faiss索引中\n",
    "    return index\n",
    "\n",
    "# 查询函数\n",
    "def query_embedding_faiss(query_data, index, k=5):\n",
    "    # 生成查询嵌入\n",
    "    query_embedding = clipEmbedding(query_data)\n",
    "    query_embedding = normalize(query_embedding)  # 归一化查询向量\n",
    "    \n",
    "    # 转换为 numpy 格式\n",
    "    query_embedding_np = query_embedding.detach().numpy()\n",
    "    \n",
    "    # 检索 Faiss 中与查询向量最相似的 k 个向量\n",
    "    D, I = index.search(query_embedding_np, k)  # D 是余弦相似度，I 是对应的索引\n",
    "\n",
    "    \n",
    "    return D, I\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
